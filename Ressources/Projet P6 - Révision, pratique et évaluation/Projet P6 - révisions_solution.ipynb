{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bd8dOd6qOZYV"
   },
   "source": [
    "# **Projet P6 - révisions et pratique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyS7Q6eAO_C4"
   },
   "source": [
    "Vous allez travailler sur des données extraites de IMDB. Cela vous permettra si vous les souhaitez (plus tard!) d'inclure ce travail à votre projet TheMoviePredictor que vous faites avec Arnaud dans lequel vous récupérez justement ces données et construisez votre base. La variable d'intérêt sera la notation IMDB des films pour pouvoir déterminer. En effet le succès commercial d'un film n'implique pas nécessairement sa qualité et il convient donc d'aller chercher plus loin que le simple profit dégagé d'une production cinématographique...\n",
    "À vous !!\n",
    "\n",
    "Les **objectifs** de ce projet sont multiples :\n",
    "1. Réviser\n",
    "2. Pratiquer\n",
    "3. Vous auto-évaluer et vous évaluer (pour nous)\n",
    "4. Vous rassurer et vous permettre de réaliser ce que vous savez faire pour pouvoir en parler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Import des librairies](#import_lib)<br>\n",
    "2. [Import des données](#import_data)<br>\n",
    "3. [Nettoyage des donnéees](#data_cleaning)<br>\n",
    "4. [Analyse exploratoire](#exploration)<br>\n",
    "5. [Pré-traitement](#preprocess)<br>\n",
    "6. [Une régression linéaire](#reglin)<br>\n",
    "7. [D'autres modèles de régression](#autres_reg)<br>\n",
    "8. [De la régression à la classification](#reg_to_class)<br>\n",
    "9. [Une régression logistique](#reglog)<br>\n",
    "10. [D'autres modèles de classification](#autre_class)<br>\n",
    "11. [En option](#option)<br>\n",
    "    11.1 [Un outil de recommandation](#reco)<br>\n",
    "    11.2 [Sauvegarder un modèle](#save)<br>\n",
    "    11.3 [Analyse en composantes principales](#acp)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import_lib'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2oUOp-mixIv"
   },
   "source": [
    "## **1. Import des libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    "> Importer dans la cellule l'ensemble des librairies nécessaires à votre travail. L'idée n'est pas de savoir immédiatement tout ce dont vous aurez besoin mais de faire des aller-retours pour y ajouter vos librairies petit à petit. L'intérêt est une meilleure lisibilité pour un lecteur extérieur qui, en quelques lignes d'import, pourra déjà avoir une idée de ce qui a été fait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_h3GW7YI3SY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **2. Import des données**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    "> Importer les données `5000_movies_bis.csv` disponible à la racine de ce document.  \n",
    "> Afficher les 7 premières lignes et **toutes** les colonnes.    \n",
    "> Répondre aux questions suivantes (répondez à toutes les questions dans une seule cellule Markdown mais évidemment le code vous ayant permis d'extraire ces informations doit être présent):\n",
    ">- combien y a-t-il d'observations/de variables ?\n",
    ">- sur combien d'années se répartissent les données ?\n",
    ">- combien de pays sont représentés ?\n",
    ">- combien de réalisateurs différents dans la base ?\n",
    ">- combien d'acteurs et d'actrices différentes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('5000_movies_bis.csv')\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour afficher toutes les colonnes on peut gérer les options d'affichage de pandas\n",
    "pd.set_option('display.max_columns', 100)\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.director_name.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(data.title_year)-min(data.title_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.country.nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = pd.Series(data[['actor_1_name','actor_2_name','actor_3_name']].values.flatten())\n",
    "actors.nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_cleaning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **3. Nettoyage des données**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    "> Vous allez dans cette partie vous occuper de faire les opérations de nettoyage sur les données. Cela implique donc de regarder en détail :\n",
    ">- les doublons\n",
    ">- les variables (à supprimer, à modifier etc...)\n",
    ">- les valeurs manquantes\n",
    ">- les zéros\n",
    "> \n",
    ">Ajouter une courte explication des décisions que vous prendrez (gestion des valeurs manquantes, suppression ou modification de certaines variables, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Les doublons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des doublons\n",
    "data = data.sort_values(by=['movie_title','num_voted_users'], ascending=[True,False])\n",
    "\n",
    "data[data.groupby('movie_title')['movie_title'].transform('size') > 1]#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des doublons\n",
    "data.drop_duplicates(subset=['director_name', 'movie_title', 'title_year'], keep=\"first\", inplace=True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Suppression de variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `plot_keywords` pourrait être utile mais sera compliquée à gérer.  \n",
    "La variable `movie_imdb_link` n'a aucun intérêt pour nous ici. Enfin elle a quand même servi pour aller scraper quelques infos supplémentaires sur IMDB mais maintenant que c'est fait, plus besoin.  \n",
    "On peut donc d'ores et déjà se séparer de ces 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['plot_keywords', 'movie_imdb_link'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Les valeurs manquantes et les zéros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(18,8))\n",
    "sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='inferno', ax=ax[0])\n",
    "ax[0].set_title('Les NaNs')\n",
    "sns.heatmap(data==0, yticklabels=False, cbar=False, cmap='inferno', ax=ax[1])\n",
    "ax[1].set_title('Les zéros');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gestion des zéros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des zéros sont des valeurs manquantes vraisemblablement, à l'exception de ceux de la variable `facenumber_in_poster`. On les remplace par des NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sélection et affichage des variables contenant des 0 sauf facenumber_in_poster\n",
    "cols = data.columns[(data==0).any()].drop('facenumber_in_poster')\n",
    "\n",
    "#remplacement des 0 par des nan\n",
    "data[cols] = data[cols].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(18,8))\n",
    "sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='inferno', ax=ax[0])\n",
    "ax[0].set_title('Les NaNs')\n",
    "sns.heatmap(data==0, yticklabels=False, cbar=False, cmap='inferno', ax=ax[1])\n",
    "ax[1].set_title('Les zéros');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gestion des NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`movie_fb_likes` et `director_fb_likes` ont trop de valeurs manquantes et supprimer les lignes diminuerait de manière trop importante la taille de notre dataset. On ne conserve donc pas ces variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression \n",
    "data.drop(['movie_fb_likes', 'director_fb_likes'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dans le dataset initial (5000_movies.csv)**, il y a beaucoup de valeurs manquantes dans `gross` et dans `budget` donc imputation délicate mais comme on veut garder ces variables, on va alors supprimer les lignes. Avant cette suppression on peut essayer de scraper IMDB pour obtenir des informations supplémentaires. Pour cela, un petit notebook séparé dans le même dossier permet de récupérer les données supplémentaires éventuelles, de les ajouter dans les colonnes `gross` et `budget` du dataframe `data`. Comme l'éxecution est un peu longue, on va sauvegarder ce dataframe dans un nouveau csv pour ne pas avoir à le refaire à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['gross', 'budget'], inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pourcentage d'observations écartées :\n",
    "round((4919-4060)/4919,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre d'observations encore incomplètes: \n",
    "4213-data.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`aspect_ratio` est la 3ème variable avec le plus de NaN. Une rapide recherche et on trouve que c'est le rapport de la largeur sur la hauteur de l'image. Probablement un intérêt limité dans notre cas mais on vérifie avant de décider si on la conserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.aspect_ratio.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principalement 2.35 et 1.85, on regroupe les autres valeurs sous une seule modalité et on va calculer la moyenne et la variance du score de chaque sous-groupe\n",
    "print(\n",
    "    'Les moyennes des 3 groupes sont :',\n",
    "    data.imdb_score[data.aspect_ratio==2.35].mean(),\n",
    "    data.imdb_score[data.aspect_ratio==1.85].mean(),\n",
    "    data.imdb_score[(data.aspect_ratio!=2.35)&(data.aspect_ratio!=1.85)].mean()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Les variances des 3 groupes sont :',\n",
    "    data.imdb_score[data.aspect_ratio==2.35].std()**2,\n",
    "    data.imdb_score[data.aspect_ratio==1.85].std()**2,\n",
    "    data.imdb_score[(data.aspect_ratio!=2.35)&(data.aspect_ratio!=1.85)].std()**2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étant données les moyennes et variances très proches des 3 groupes, on peut supprimer cette variable de notre analyse sans que cela affecte les résultats.\n",
    "data.drop('aspect_ratio', axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_rating` est la 4ème variable avec le plus de NaN. On souhaite conserver cette variable en revanche on pourra difficilement faire de l'imputation de valeurs qui a du sens donc on choisit de supprimer les lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['content_rating'], inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder certaines variables \"à la main\" car on pourra trouver les informations facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facenumber_in_poster\n",
    "data[data.facenumber_in_poster.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il y en a que 9, exceptionnellement on peut aller voir les affiches des films ça va aller vite\n",
    "data.loc[99,'facenumber_in_poster'] = 1 #hobbit\n",
    "data.loc[248,'facenumber_in_poster'] = 8 #tortues ninja\n",
    "data.loc[1948,'facenumber_in_poster'] = 2 #dear john\n",
    "data.loc[3016,'facenumber_in_poster'] = 2 #heaven\n",
    "data.loc[3373,'facenumber_in_poster'] = 0 #kicks\n",
    "data.loc[3797,'facenumber_in_poster'] = 0 #the visit\n",
    "data.loc[3853,'facenumber_in_poster'] = 6 #mom's night out\n",
    "data.loc[4444,'facenumber_in_poster'] = 4 #Growing Up Smith\n",
    "data.loc[4692,'facenumber_in_poster'] = 4 #The Sisterhood of Night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color\n",
    "data[data.color.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les 4 valeurs manquantes correspondent à des films récents et en couleur\n",
    "data.loc[data.color.isna(), 'color'] = 'Color'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language\n",
    "data[data.language.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les 3 valeurs manquantes correspondent à des films américains\n",
    "data.loc[data.language.isna(), 'language'] = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title_year\n",
    "data[data.title_year.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 valeur manquante pour le film Carlos qui date de 2010\n",
    "data.loc[2466, 'title_year'] = 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde un peu plus en détails les histoires de likes facebook. Notamment la correlation entre ces variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['cast_total_fb_likes', 'actor_1_fb_likes', 'actor_2_fb_likes', 'actor_3_fb_likes', 'imdb_score']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cast_total_fb_likes` et `actor_1_fb_likes` sont très corrélées. Pour les 2 autres variables `actor_2_fb_likes` et `actor_3_fb_likes` on peut choisir de les supprimer pour les remplacer par une information `other_actors_fb_likes` = `cast_total_fb_likes` - `actor_1_fb_likes`, cela permettra de diminuer le nombre de valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de other_actors_fb_likes\n",
    "data['other_actors_fb_likes'] = data.cast_total_fb_likes - data.actor_1_fb_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression variables redondantes\n",
    "data.drop(['cast_total_fb_likes', 'actor_2_fb_likes', 'actor_3_fb_likes'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, pour les 6 lignes ayant des valeurs manquantes restantes pour les likes facebook on peut imputer la moyenne.  \n",
    "Idem pour num_critic_for_reviews, num_user_for_reviews et duration.  \n",
    "Quant aux noms d'acteurs/réalisateurs, comme on ne les gardera pas pour la partie modélisation mais uniquement pour la visualisation (on reviendra dessus le moment venu), on peut les conserver tels quels pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation de la moyenne pour quelques valeurs manquantes\n",
    "data.loc[data.other_actors_fb_likes.isna(),'other_actors_fb_likes'] = data.other_actors_fb_likes.mean()\n",
    "data.loc[data.actor_1_fb_likes.isna(),'actor_1_fb_likes'] = data.actor_1_fb_likes.mean()\n",
    "data.loc[data.num_critic_for_reviews.isna(),'num_critic_for_reviews'] = data.num_critic_for_reviews.mean()\n",
    "data.loc[data.num_user_for_reviews.isna(),'num_user_for_reviews'] = data.num_user_for_reviews.mean()\n",
    "data.loc[data.duration.isna(),'duration'] = data.duration.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on reset l'index après toutes les suppressions de lignes\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modification de variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des valeurs pour voir s'il n'y a pas de caractères spéciaux qui se baladent.\n",
    "data.loc[64].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c'est pas évident à voir mais il y a bien un \\xa0 louche après les titres des films\n",
    "data.movie_title = data.movie_title.apply(lambda row : row.replace('\\xa0',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va regarder plus en détail la variable `content_rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.content_rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une petite recherche et on trouve que historiquement, on a plus ou moins :\n",
    "- Passed = Approved = M = GP = PG\n",
    "- TV-14 = PG-13\n",
    "- X = NC-17\n",
    "- Not Rated = Unrated = NR\n",
    "\n",
    "On veut donc remplacer :\n",
    "- Passed, Approved, M et GP par **PG**\n",
    "- TV-14 par **PG-13**\n",
    "- X par **NC-17**\n",
    "- Not Rated et Unrated par **UR** qui sont les notations utilisées aujourd'hui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rating(row):\n",
    "    if row['content_rating'] in ['Passed', 'Approved', 'M', 'GP']:\n",
    "        return 'PG'\n",
    "    elif row['content_rating'] in ['Not Rated', 'Unrated']:\n",
    "        return 'UR'\n",
    "    elif row['content_rating'] == 'X':\n",
    "        return 'NC-17'\n",
    "    elif row['content_rating'] == 'TV-14':\n",
    "        return 'PG-13'\n",
    "    else:\n",
    "        return row['content_rating']\n",
    "    \n",
    "data['content_rating'] = data.apply(replace_rating, axis=1)\n",
    "data.content_rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a déjà géré presque tout, le dernier point en suspens est la variable `genre` qu'il faut spliter puis créer des OneHotEncoded variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dummies = data.genres.str.get_dummies('|')\n",
    "data = pd.concat([data,genre_dummies], axis=1)\n",
    "data.drop(['genres'], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_language(row):\n",
    "    if row['language'] in ['French', 'Spanish', 'German', 'Italian', 'Portuguese', 'Norwegian', 'Dutch',\n",
    "                           'Danish', 'Romanian', 'Bosnian', 'Czech', 'Hungarian', 'Swedish']:\n",
    "        return 'European'\n",
    "    elif row['language'] == 'English':\n",
    "        return 'English'\n",
    "    else:\n",
    "        return 'Other languages'\n",
    "    \n",
    "data['language'] = data.apply(replace_language, axis=1)\n",
    "data.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_country(row):\n",
    "    if row['country'] in ['UK', 'France', 'Spain', 'Germany', 'West Germany', 'Italy', 'Portugal', 'Norway', 'Netherlands',\n",
    "                        'Denmark', 'Ireland', 'Romania', 'Iceland', 'Czech', 'Hungary', 'Sweden', 'Belgium', 'Greece',\n",
    "                        'Bulgaria', 'Switzerland', 'Poland', 'Finland']:\n",
    "        return 'Europe'\n",
    "    elif row['country'] in ['USA', 'Canada']:\n",
    "        return 'North America'\n",
    "    else:\n",
    "        return 'Other countries'\n",
    "    \n",
    "data['country'] = data.apply(replace_country, axis=1)\n",
    "data.country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **4. Analyse exploratoire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, vous devez \"explorer\" vos données. Cette tâche, qui peut s'avérer très vaste, consiste à s'intéresser à l'information contenue dans nos données \"au premier abord\".\n",
    "\n",
    "Sont donc attendus dans cette partie :\n",
    ">- quelques statistiques descriptives\n",
    ">- entre 6 et 10 visualisations (vous pouvez bien sûr en regrouper plusieurs sur une même figure)\n",
    ">- et pour chaque résultat/graphique présenté, une explication succinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "LWdg0QamqEdi",
    "outputId": "303b5a58-077f-4a72-cfae-b2708737fd52"
   },
   "outputs": [],
   "source": [
    "#historique des sorties de films\n",
    "data.hist('title_year', bins=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Les moyennes par genre\n",
    "mean_genre = {}\n",
    "for gen in data.columns[-21:]:\n",
    "    mean_genre[gen] = data.loc[data[gen]==1,'imdb_score'].mean()\n",
    "    \n",
    "plt.bar(mean_genre.keys(), mean_genre.values())\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "colab_type": "code",
    "id": "m6IDOUpy2Y_t",
    "outputId": "b88a9979-78cb-4447-e8f8-ae7de357c34d"
   },
   "source": [
    "On peut regarder un peu le profit généré par les films pour cela on crée la variable `profit` = `gross` - `budget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gunQra7ckaaQ",
    "outputId": "5b617f5d-f22f-4462-e0b3-4e221bf02764"
   },
   "outputs": [],
   "source": [
    "#Top 20 des films ayant généré le plus de profit\n",
    "data['profit'] = data.gross - data.budget\n",
    "top20profit = data[['movie_title', 'profit', 'budget']].sort_values('profit', ascending=False).iloc[:20].reset_index(drop=True)\n",
    "top20profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(23,20))\n",
    "top20profit.plot('budget', 'profit', kind='scatter', ax=ax, s=100, linewidth=0, c=range(20), colormap=\"plasma\", colorbar=False);\n",
    "\n",
    "for k in range(20):\n",
    "    ax.annotate(top20profit.loc[k,'movie_title'], top20profit.loc[k,['budget','profit']],\n",
    "                xytext=(10,-5), textcoords='offset points',\n",
    "                family='sans-serif', fontsize=13, color='darkslategrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retour sur investissement du top20 des profits\n",
    "top20profit['roi'] = top20profit.profit/top20profit.budget*100\n",
    "top20profit = top20profit.sort_values('roi', ascending=False).reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,12))\n",
    "top20profit.plot('budget', 'roi', kind='scatter', ax=ax, s=100, linewidth=0, c=range(20), colormap=\"plasma\", colorbar=False);\n",
    "for k in range(20):\n",
    "    ax.annotate(top20profit.loc[k,'movie_title'], top20profit.loc[k,['budget','roi']],\n",
    "                xytext=(10,-5), textcoords='offset points',\n",
    "                family='sans-serif', fontsize=13, color='darkslategrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 des films ayant les meilleurs retour sur investissement\n",
    "data['roi'] = data.profit/data.budget*100\n",
    "top10roi = data[['movie_title', 'roi', 'budget']].sort_values('roi', ascending=False).iloc[:10].reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,12))\n",
    "top10roi.plot('budget', 'roi', kind='scatter', ax=ax, s=100, linewidth=0, c=range(10), colormap=\"plasma\", colorbar=False);\n",
    "for k in range(10):\n",
    "    ax.annotate(top10roi.loc[k,'movie_title'], top10roi.loc[k,['budget','roi']],\n",
    "                xytext=(10,-5), textcoords='offset points',\n",
    "                family='sans-serif', fontsize=13, color='darkslategrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 des réalisateurs ayant les meilleurs notes IMDB et leur nombre de films dans la base\n",
    "directors = data[['director_name', 'imdb_score', 'movie_title']].groupby('director_name').agg({'imdb_score':'mean', 'movie_title':'count'})\n",
    "top20real = directors.sort_values('imdb_score', ascending=False).iloc[:20]\n",
    "top20real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocess'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **5. Pré-traitement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Maintenant que vous commencez à bien connaître votre base de données, on va la préparer pour la partie modélisation.\n",
    ">\n",
    ">Sont donc attendus dans cette partie :\n",
    ">- restriction aux données utiles à la prédiction : potentiellement certaines variables conservées pour la visualisation sont à supprimer pour la modélisation\n",
    ">- création des échantillons d'entraînement et de test\n",
    ">- gestion des variables catégoriques d'un côté et numériques de l'autre\n",
    ">\n",
    ">La standardisation n'étant pas toujours nécessaire puisque ça dépend des modèles, vous pouvez choisir de la faire dès maintenant ou bien d'attendre de voir si vous en avez besoin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppression de variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer on peut ensuite s'interroger sur l'intérêt de conserver les noms des acteurs/réalisateurs dans les variables car il y en a énormément (2399 réalisateurs et 6256 acteurs différents). Cette très forte variabilité limite l'impact de ces variables on va donc supprimer ces variables.\n",
    "\n",
    "On peut aussi déjà virer la variable `profit` qu'on a créé pour la partie visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['director_name', 'actor_2_name', 'actor_1_name', 'movie_title', 'actor_3_name', 'profit', 'roi'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables catégoriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va maintenant créer les dummy variables\n",
    "for varcat in data.select_dtypes(include=\"object\").columns :\n",
    "    dum = pd.get_dummies(data[varcat], drop_first=True)\n",
    "    data = pd.concat([data,dum], axis=1)\n",
    "    data.drop([varcat], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables numériques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on va utiliser vraisemblablement différents modèles dont certains nécéssitent un *feature scaling*, on va le faire dès maintenant (enfin une fois qu'on aura découpé nos données en échantillons *train* et *test* puisque, pour rappel, les données *test* doivent être transformé de la même manière que les données *train*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Création des échantillons pour la modélisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrice X et vecteur y\n",
    "X = data.drop('imdb_score', axis=1)\n",
    "y = data['imdb_score']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reglin'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **6. Une régression linéaire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Tout est dans le titre. Vous devez ici entraîner et tester une régression linéaire pour la prédiction de la note IMDB.  \n",
    ">Par ailleurs, sont attendus ici :\n",
    ">- un affichage et une interprétation des coefficients et de leur significativité\n",
    ">- le choix d'une mesure d'évaluation du modèle et son interprétation\n",
    ">- une validation croisée pour l'estimation de la qualité du modèle\n",
    ">- *facultatif : l'ajout d'une régularisation Ridge ou Lasso pour déterminer si les résultats sont meilleurs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# échantillons train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_train.columns, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour pouvoir obtenir simplement les coefficients et les p-values, on passe par OLS de statsmodel\n",
    "X_train_const = add_constant(X_train) # pour ajouter une constante à notre modèle sinon il n'y en a pas par défaut\n",
    "\n",
    "# on fit la régression linéaire sur les données d'entrainement et on affiche direct le summary\n",
    "OLS(y_train, X_train_const).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mean_squared_error(OLS(y_train,X_train_const).fit().predict(X_train_const),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE sur X_test\n",
    "X_test_const = add_constant(X_test)\n",
    "mean_squared_error(OLS(y_train,X_train_const).fit().predict(X_test_const),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut aussi utiliser RFE pour sélection les variables : disons qu'on veut en garder en 20\n",
    "rfe = RFE(LinearRegression(), 20)\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rfe = X_train.columns[rfe.support_]\n",
    "col_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colonnes supprimées\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée un jeu de données RFE avec uniquement les variables sélectionnées via RFE\n",
    "X_train_rfe = X_train[col_rfe]\n",
    "X_test_rfe = X_test[col_rfe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS(y_train, add_constant(X_train_rfe)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE sur X_train_rfe\n",
    "print('train', mean_squared_error(OLS(y_train,add_constant(X_train_rfe)).fit().predict(add_constant(X_train_rfe)),y_train))\n",
    "print('test', mean_squared_error(OLS(y_train,add_constant(X_train_rfe)).fit().predict(add_constant(X_test_rfe)),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on pouvait s'y attendre, la régression linéaire n'est vraiment pas terrible puisque le modèle n'est pas linéaire...on peut essayer une regression régularisée mais on est pas non plus en situation de sur-apprentissage donc l'intérêt est limité. Pour les avoir testées, les régressions Ridge et Lasso ne donnent pas non plus de bons résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='autres_reg'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **7. D'autres modèles de régression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Tout est encore dans le titre. Mettez en place le modèle **de régression** que vous souhaitez.  \n",
    ">Sont donc attendus dans cette partie :\n",
    ">- une petite phrase pour justifier votre choix\n",
    ">- les pré-traitements supplémentaires nécessaires s'il y en a\n",
    ">- évaluation du modèle avec `cross_val_score` ou `cross_validate`\n",
    ">- affinage des éventuels hyperparamètres avec `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SVM avec différents kernels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf = SVR(kernel='rbf', gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', gamma='auto')\n",
    "svr_poly = SVR(kernel='poly', gamma='auto', degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf.fit(X_train, y_train)\n",
    "y_pred_svm_rbf = svr_rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm_rbf.min(), y_pred_svm_rbf.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_pred_svm_rbf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_lin.fit(X_train, y_train)\n",
    "y_pred_svm_lin = svr_lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm_lin.min(), y_pred_svm_lin.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_pred_svm_lin, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_poly.fit(X_train_rfe, y_train)\n",
    "y_pred_svm_poly = svr_poly.predict(X_test_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm_poly.min(), y_pred_svm_poly.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_pred_svm_poly, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientboost = GradientBoostingRegressor(loss='ls',learning_rate=0.03,n_estimators=200,max_depth=4)\n",
    "gradientboost.fit(X_train_rfe,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = gradientboost.predict(X_test_rfe)\n",
    "error = gradientboost.loss_(y_test,y_pred_gb) # la fonction de perte est Mean square error\n",
    "print(\"MSE:%.3f\" % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_pred_gb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb.min(), y_pred_gb.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recherche des hyperparametres avec GridSearchCV\n",
    "param_grid = {\n",
    "    'loss' : ['ls'],\n",
    "    'max_depth' : [6,7,8],\n",
    "    'learning_rate' : [0.01],\n",
    "    'n_estimators': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(GradientBoostingRegressor(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_gb.fit(X_train, y_train)\n",
    "grid_search_gb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_gb_pred = grid_search_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test.values, grid_search_gb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Forêts aléatoires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators = 500)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_pred = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(rfr_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [10, 50, 100, 150],\n",
    "    'n_estimators': [100, 500, 1000, 1500]\n",
    "}\n",
    "\n",
    "grid_search_rfr = GridSearchCV(RandomForestRegressor(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rfr.fit(X_train, y_train)\n",
    "grid_search_rfr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grid_pred_rfr = grid_search_rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_grid_pred_rfr, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators = 500)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(xgb.predict(X_test), y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2, 5, 10, 15],\n",
    "    'learning_rate' : [0.001, 0.01, 0.1, 1],\n",
    "    'n_estimators' : [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(XGBRegressor(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgb.fit(X_train, y_train)\n",
    "grid_search_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators = 500)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = grid_search_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test.values, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='irr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interprétation des résultats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va conserver le modèle XG Boost qui obtient le plus faible MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = grid_search_xgb.best_estimator_.feature_importances_\n",
    "sorted_importance = np.argsort(feature_importance)\n",
    "pos = np.arange(len(sorted_importance))\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(pos, feature_importance[sorted_importance],align='center')\n",
    "plt.yticks(pos, X_train.columns[sorted_importance],fontsize=15)\n",
    "plt.title('Feature Importance ',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reg_to_class'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **8. De la régression à la classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Transformez le problème de régression en un problème de classification par une discrétisation du score IMDB en 5 classes : nul, bof, sympa, bon, super.  \n",
    ">Justifiez votre découpage en indiquant quels seuils vous avez utilisé et pourquoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_class = pd.qcut(y, 5, labels=['nul','bof','sympa','bon','super'])\n",
    "y_class = pd.cut(y, [0,4,6,7,8,10], labels=[0,1,2,3,4])\n",
    "y_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size = 0.2, random_state=64)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_train.columns, index = X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reglog'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **9. Une régression logistique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Vous devez ici entraîner et tester une régression logistique pour la prédiction de la classe du film.  \n",
    ">Par ailleurs, sont attendus ici :\n",
    ">- un affichage et une interprétation des *Odds-ratio* et de leur significativité\n",
    ">- le choix d'une ou plusieurs mesures d'évaluation du modèle et leur interprétation\n",
    ">- une validation croisée pour l'évaluation modèle\n",
    ">- l'affinage des hyperparamètres avec l'outil qui va bien\n",
    ">- peut-on tracer les courbes ROC et calculer l'AUC ? Pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglog = LogisticRegression(solver='lbfgs')\n",
    "reglog.fit(X_train,y_train)\n",
    "print(reglog.score(X_train,y_train), reglog.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(reglog.predict(X_test),y_test), annot=True, fmt='d', cbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut aussi utiliser RFE pour sélection les variables : disons qu'on veut en garder en 25\n",
    "rfe = RFE(LogisticRegression(solver='lbfgs'), 25)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "col_rfe = X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colonnes supprimées\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée un jeu de données RFE avec uniquement les variables sélectionnées via RFE\n",
    "X_train_rfe = X_train[col_rfe]\n",
    "X_test_rfe = X_test[col_rfe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglog = LogisticRegression(solver='lbfgs')\n",
    "reglog.fit(X_train_rfe,y_train)\n",
    "print(reglog.score(X_train_rfe,y_train), reglog.score(X_test_rfe,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='autre_class'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **10. Un autre modèle de classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Au choix, une autre méthode de classification. Évidemment, sentez-vous libre d'en essayer plus d'une et de les comparer.  \n",
    ">Sont donc attendus dans cette partie :\n",
    ">- une petite phrase pour justifier votre choix\n",
    ">- les pré-traitements supplémentaires nécessaires s'il y en a\n",
    ">- évaluation du modèle\n",
    ">- étude de l'importance des paramètres, si votre modèle le permet\n",
    ">- affinage des éventuels hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SVM avec différents kernels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel='linear', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)\n",
    "svc_poly = SVC(kernel='poly', C=100, gamma= 'scale', degree = 3, decision_function_shape='ovo', random_state = 42)\n",
    "svc_rbf = SVC(kernel='rbf', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_linear.fit(X_train, y_train)\n",
    "print(classification_report(y_test, svc_linear.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_poly.fit(X_train, y_train)\n",
    "print(classification_report(y_test, svc_poly.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_rbf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, svc_rbf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Forêts aléatoires pour la classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [50, 100, 150],\n",
    "    'n_estimators': [100, 500, 1000, 1500],\n",
    "    'random_state' :[0]\n",
    "}\n",
    "\n",
    "grid_search_rfc = GridSearchCV(RandomForestClassifier(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search_rfc.fit(X_train, y_train)\n",
    "print(classification_report(y_test, grid_search_rfc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradient Boost pour la classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [10, 50, 90],\n",
    "    'max_features': [3],\n",
    "    'min_samples_leaf': [3],\n",
    "    'min_samples_split': [8, 10],\n",
    "    'n_estimators': [100, 500],\n",
    "    'learning_rate' : [0.1, 0.2],\n",
    "    'random_state' : [0]\n",
    "}\n",
    "\n",
    "grid_search_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search_gbc.fit(X_train, y_train)\n",
    "print(classification_report(y_test, grid_search_gbc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XG Boost pour la classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "     'objective' : ['multi:softmax', 'multi:softprob'],\n",
    "     'n_estimators': [100, 500, 1000],\n",
    "     'random_state': [0]\n",
    "}\n",
    "\n",
    "grid_search_xgbc = GridSearchCV(XGBClassifier(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search_xgbc.fit(X_train, y_train)\n",
    "print(classification_report(y_test, grid_search_xgbc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interprétation des résultats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va conserver le modèle XG Boost qui obtient la meilleure *accuracy* sur l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = grid_search_xgbc.best_estimator_.feature_importances_\n",
    "sorted_importance = np.argsort(feature_importance)\n",
    "pos = np.arange(len(sorted_importance))\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.barh(pos, feature_importance[sorted_importance],align='center')\n",
    "plt.yticks(pos, X_train.columns[sorted_importance],fontsize=15)\n",
    "plt.title('Feature Importance ',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='option'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "## **11. En option**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo, si vous êtes arrivés jusqu'ici !!!\n",
    "\n",
    "Pour les flèches, hésitez pas à continuer si vous en voulez encore et pour les autres, hésitez pas à y revenir à l'occasion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reco'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "### **11.1. Un outil de recommandation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Question un peu plus ouverte pour terminer: en utilisant une méthode de clustering (donc d'apprentissage non-supervisé), construisez un petit outil de recommandation de films.  \n",
    ">Pour un film donné, votre méthode doit donc retourner les films qui lui ressemblent le plus.  \n",
    ">Pour rappel, on avait fait un petit exercice comme celui-cilorsqu'on avait vu les *k-plus proches voisins*, donc vous êtes invités à ne pas utiliser kNN, sinon c'est pas drôle...  \n",
    ">Vous pourrez bientôt aller plus loin en créant une petite application web permettant une interface pour choisir un film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='save'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "### **11.2. Sauvegarder un modèle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Utilisez le module `pickle` pour sauvegarder le meilleur de vos modèles et le recharger ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNnaPdClkRvC"
   },
   "source": [
    "### **11.3. Analyse en Composantes Principales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À FAIRE**\n",
    "\n",
    ">Utilisez une ACP pour visualiser vos données en dimension 2 ou 3 avec des points dont la couleur varie en fonction de la classe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "US Income Final.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
