{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les k-plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les données puis on cherche à prédire la valeur $y^{(0)}$ pour l'observation $x^{(0)} = (x^{(0)}_i)_{i \\in [1,p]}$.  \n",
    "Dans le cas de la classification, cela revient à avoir un $y^{(0)}$ catégorique où les modalités sont les classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Initialiser $k$ et choisir une fonction de distance\n",
    ">2. Pour chaque observation $x^{(i)}$ dans les données:\n",
    ">>a. Calculer la distance $d$ entre $x^{(0)}$ et $x^{(i)}$  \n",
    ">>b. Stocker la distance $d$ et l’indice $i$ de l’observation $x^{(i)}$ (dans une liste de couples par exemple)\n",
    ">4. Trier la liste contenant distances et indices de la plus petite distance à la plus grande (dans ordre croissant).\n",
    ">5. Sélectionner les $k$ premiers éléments\n",
    ">6. Obtenir les étiquettes des $k$ entrées sélectionnées\n",
    ">7. Si **régression**, retourner la moyenne des $k$ valeurs $y^{(i)}$ ; si **classification**, retourner le mode des $k$ classes $y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémenter l'algorithme des k plus proches voisins dans le cas univarié. Tester cet algorithme sur les données ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "'''Calcul de la moyenne des k valeurs de y correspondant aux k plus proches voisins'''\n",
    "def moyenne(y_kNN):\n",
    "    return sum(y_kNN) / len(y_kNN)\n",
    "\n",
    "'''Calcul du mode des k valeurs de y correspondant aux k plus proches voisins'''\n",
    "def mode(y_kNN):    \n",
    "    liste = sorted(set(y_kNN),key=y_kNN.count)      \n",
    "    return liste[-1]\n",
    "\n",
    "'''Calcul de la distance euclidienne'''\n",
    "def distance_euclidienne(a, b):      \n",
    "    return sqrt((a-b)**2)\n",
    "\n",
    "'''Algorithme des k plus proches voisins kNN'''\n",
    "def knn(dataX, dataY, x_0, k=3, distance=distance_euclidienne, pred=mode):\n",
    "    \n",
    "    #initialisation de la liste des voisins\n",
    "    voisins = []\n",
    "    \n",
    "    #parcours des données et ajout des couples (distances,indices) dans la liste voisins\n",
    "    for i in range(len(dataX)):\n",
    "        d = distance(dataX[i], x_0)\n",
    "        voisins.append((d, i))\n",
    "        \n",
    "    #tri de la liste voisins\n",
    "    voisins = sorted(voisins)\n",
    "    \n",
    "    #identification des k-PPV\n",
    "    kNN = voisins[:k]\n",
    "    \n",
    "    #récupération des y correspondant aux k-PPV\n",
    "    y_kNN = [dataY[couple[1]] for couple in kNN]\n",
    "    \n",
    "    return kNN , pred(y_kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données de régression\n",
    "# Colonne 0: taille (cm)\n",
    "# Colonne 1: poids (kg)\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "reg_data = np.array([\n",
    "    [167. ,  50.8],\n",
    "    [181.7,  61.4],\n",
    "    [176.3,  68.9],\n",
    "    [173.3,  64.1],\n",
    "    [172.2,  64.9],\n",
    "    [174.5,  55.5],\n",
    "    [177.3,  63.7],\n",
    "    [177.8,  61.4],\n",
    "    [172.5,  50.6],\n",
    "    [168.9,  57.4]])\n",
    "\n",
    "k=3\n",
    "x0=170\n",
    "dataX=reg_data[:,0]\n",
    "dataY=reg_data[:,1]\n",
    "\n",
    "voisins = [] #list()\n",
    "for i in range(len(dataX)) :\n",
    "    d = sqrt((x0-dataX[i])**2)\n",
    "    voisins.append((d,i))\n",
    "    #voisins.append((i,d))\n",
    "#pour trier la liste selon 2ème élément du couple\n",
    "#voisins.sort(key= lambda couple : couple[1])\n",
    "\n",
    "voisins.sort() #voisins = sorted(voisins)\n",
    "kppv = voisins[:k]\n",
    "\n",
    "y_ppv = []\n",
    "for voisin in range(k):\n",
    "    y_ppv.append(dataY[kppv[voisin][1]])\n",
    "\n",
    "y_ppv = [dataY[kppv[v][1]] for v in range(k)]\n",
    "\n",
    "sum(y_ppv)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(reg_data[:,0], reg_data[:,1], 170, k=3, distance=distance_euclidienne, pred=moyenne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données de Classification\n",
    "# Colonne 0: age\n",
    "# Colonne 1: aime l'ananas dans la pizza\n",
    "\n",
    "import numpy as np\n",
    "clf_data = np.array([\n",
    "   [22, 1],\n",
    "   [23, 1],\n",
    "   [21, 1],\n",
    "   [18, 1],\n",
    "   [19, 1],\n",
    "   [25, 0],\n",
    "   [27, 0],\n",
    "   [29, 0],\n",
    "   [31, 0],\n",
    "   [45, 0],\n",
    "   [23, 0]\n",
    "])\n",
    "\n",
    "k=3\n",
    "x0=30\n",
    "dataX=clf_data[:,0]\n",
    "dataY=clf_data[:,1]\n",
    "\n",
    "voisins = [] #list()\n",
    "for i in range(len(dataX)) :\n",
    "    d = sqrt((x0-dataX[i])**2)\n",
    "    voisins.append((d,i))\n",
    "    #voisins.append((i,d))\n",
    "#pour trier la liste selon 2ème élément du couple\n",
    "#voisins.sort(key= lambda couple : couple[1])\n",
    "\n",
    "voisins.sort() #voisins = sorted(voisins)\n",
    "kppv = voisins[:k]\n",
    "\n",
    "y_ppv = [dataY[kppv[v][1]] for v in range(k)]\n",
    "\n",
    "liste = sorted(set(list(dataY)),key=list(dataY).count)      \n",
    "liste[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(clf_data[:,0], clf_data[:,1], 33, k=3, distance=distance_euclidienne, pred=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Généraliser à des données multivariées et tester sur le dataset iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En 1936, Edgar Anderson a collecté des données sur 3 espèces d'iris : \"iris setosa\", \"iris virginica\" et \"iris versicolor\"\n",
    "<img src=\"iris_setosa.jpeg\"><img src=\"iris_virginica.jpeg\"><img src=\"iris_versicolor.jpeg\">\n",
    "\n",
    "Pour chaque iris étudié, Anderson a mesuré (en cm) :\n",
    "- la largeur des sépales\n",
    "- la longueur des sépales\n",
    "- la largeur des pétales\n",
    "- la longueur des pétales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Iris.csv')\n",
    "data = data.sample(frac=1) #pour ne pas avoir toutes les fleurs d'une même classe \"rangées ensemble\"\n",
    "# cela a une importance dans notre algo car on teste les voisins dans l'ordre du dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "'''Calcul de la moyenne des k valeurs de y correspondant aux k plus proches voisins'''\n",
    "def moyenne(y_kNN):\n",
    "    return sum(y_kNN) / len(y_kNN)\n",
    "\n",
    "'''Calcul du mode des k valeurs de y correspondant aux k plus proches voisins'''\n",
    "def mode(y_kNN):    \n",
    "    liste = sorted(set(y_kNN),key=y_kNN.count)      \n",
    "    return liste[-1]\n",
    "\n",
    "'''Calcul de la distance euclidienne'''\n",
    "def distance_euclidienne(a, b):\n",
    "    s = 0\n",
    "    for i in range(len(a)):\n",
    "        s += (a[i] - b[i])**2       \n",
    "    return sqrt(s)\n",
    "    #return sqrt(sum((a-b)**2))\n",
    "\n",
    "'''Algorithme des k plus proches voisins kNN'''\n",
    "def kNN(dataX, dataY, x_0, k=3, distance=distance_euclidienne, pred=mode):\n",
    "    \n",
    "    #initialisation de la liste des voisins\n",
    "    voisins = []\n",
    "    \n",
    "    #parcours des données et ajout des couples (distances,indices) dans la liste voisins\n",
    "    #legère modification car pbm d'indexes avec les échantillons train/test par la suite\n",
    "    #cela permet ainsi d'utiliser les noms de films comme index pour la partie recoomandation de films\n",
    "    for i in dataX.index: \n",
    "        d = distance(dataX.loc[i], x_0)\n",
    "        voisins.append((d, i))\n",
    "        \n",
    "    #tri de la liste voisins\n",
    "    voisins = sorted(voisins)\n",
    "    \n",
    "    #identification des k-PPV\n",
    "    kNN = voisins[:k]\n",
    "    \n",
    "    #récupération des y correspondant aux k-PPV\n",
    "    y_kNN = [dataY[couple[1]] for couple in kNN]\n",
    "\n",
    "    return kNN , pred(y_kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = data.drop(['Name'], axis = 1)\n",
    "#dataX = data[['SepalLength','SepalWidth']]\n",
    "dataY = data['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN(dataX, dataY, [7,2,4,1], k=20, distance=distance_euclidienne, pred=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mesurer le score du modèle pour un k donné (en utilisant un jeu de données test) puis comparer ce score pour différents k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataX, dataY,\n",
    "    test_size=0.2, random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [] #liste dans laquelle on stocke le pourcentage de bonnes classifications du modèle pour un k donné\n",
    "for k in range(1,31):\n",
    "#calcul des prédictions pour l'ensemble de X de l'échantillon de test\n",
    "    pred = []\n",
    "    for i,r in X_test.iterrows():\n",
    "        pred.append(kNN(X_train, y_train, r, k=k, distance=distance_euclidienne, pred=mode)[1])\n",
    "        #calcul du score du modèle = pourcentage de bonnes prédictions\n",
    "    scores.append((pred==y_test).sum()/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1,31),scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utiliser votre algorithme des kNN pour effectuer des recommendations de films : pour un film donné, l'algorithme doit renvoyer les 5 films les plus \"proches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "films = pd.read_csv('Movie-Ratings.csv')\n",
    "#on peut renommer les colonnes pour que ce soit plus simple d'utilisation\n",
    "films.columns = ['Film', 'Genre', 'RT', 'Audience', 'Budget', 'Year']\n",
    "#on peut choisir de travailler avec les noms de films comme index\n",
    "films.set_index('Film',inplace=True)\n",
    "films.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on créé des dummy pour la variable 'Genre'\n",
    "dummy= pd.get_dummies(films['Genre'])\n",
    "films = pd.concat((films,dummy), axis=1)\n",
    "films.drop('Genre', axis=1, inplace=True)\n",
    "films.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut choisir de supprimer l'année si on considère que ce n'est pas intéressant pour nos recommandations\n",
    "# vous pouvez tout à fait décider de garder cette info, à vous de voir !\n",
    "films.drop('Year', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut normaliser les notes, les budgets et l'année entre 0 et 1 car les différences des genres valent 0 ou 1\n",
    "films.RT = (films.RT - min(films.RT))/(max(films.RT) - min(films.RT))\n",
    "films.Audience = (films.Audience - min(films.Audience))/(max(films.Audience) - min(films.Audience))\n",
    "films.Budget = (films.Budget - min(films.Budget))/(max(films.Budget) - min(films.Budget))\n",
    "\n",
    "films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on appelle notre algo kNN pour récupérer les plus proches voisins et donc les recommandations\n",
    "from numpy.random import randint\n",
    "film_au_hasard = films.index[randint(len(films))]\n",
    "\n",
    "reco = kNN(films, films.Action, films.loc[film_au_hasard], k=6, distance=distance_euclidienne, pred=mode)[0]\n",
    "\n",
    "films.loc[[f for d,f in reco]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintenant que vous l'avez bien compris, implémenté, testé et validé, vous pouvez chercher les implémentations existantes de cet algorithme dans Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Iris.csv')\n",
    "dataX = data.drop(['Name'], axis = 1)\n",
    "dataY = data['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.2, random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "scores = []\n",
    "\n",
    "for k in range(1,31):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "    knn.fit(X_train, y_train)\n",
    "    #pred = knn.predict(X_test)\n",
    "    scores.append(knn.score(X_test,y_test))\n",
    "    \n",
    "plt.plot(range(1,31),scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
