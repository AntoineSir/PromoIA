{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Une régression logistique sur la couleur d'un vin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cet exercice est de prédire la couleur d'un vin à partir de ses composants et visualiser la performance avec une courbe ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez allez chercher les données [là](https://archive.ics.uci.edu/ml/datasets/wine+quality).  \n",
    "Vous verrez que l'on a 2 tables : une pour les vins rouges et une pour les vins blancs. La première étape consistera donc à fusionner ces deux datasets pour en avoir un seul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reds = pd.read_csv('winequality-red.csv',sep=\";\")\n",
    "reds[\"color\"]='red'\n",
    "reds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whites = pd.read_csv('winequality-white.csv', sep=\";\")\n",
    "whites[\"color\"] = 'white'\n",
    "whites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wines = pd.concat([reds,whites],axis=0)\n",
    "wines.reset_index(drop=True, inplace=True)\n",
    "wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un peu de dataviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques graphiques pour visualiser les distributions des différentes variables indépendantes selon la couleur du vin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,18))\n",
    "ax = []\n",
    "for i in range(10):\n",
    "    ax.append(fig.add_subplot(4,3,i+1))\n",
    "    sns.boxplot(x='color',y=wines.columns[i],data=wines,palette='winter',ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un train set et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wines.drop(['quality', 'color'], axis=1)\n",
    "y = wines['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création et entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "reglog = LogisticRegression()\n",
    "reglog.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première façon de vérifier que le modèle a marché consiste à regarder la matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, reglog.predict(X_test))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut faire un affichage plus \"joli\" avec un DataFrame\n",
    "cm = pd.DataFrame(cm, columns=['prédit ' + _ for _ in reglog.classes_])\n",
    "cm.index = ['vrai ' + _ for _ in reglog.classes_]\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un classifieur construit une frontière entre deux classes, la distance d'un point à la frontière constitue une information importante. Plus elle est grande, plus le modèle est confiant. Cette distance est souvent appelée *score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = reglog.decision_function(X_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais on préfère les probabilités quand elles sont disponibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = reglog.predict_proba(X_test)\n",
    "probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons comment le score est distribué :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pd.DataFrame(score, columns=['score'])\n",
    "sc['color'] = y_test.values\n",
    "sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sc['score'].hist(bins=50, figsize=(8,4))\n",
    "ax.set_title('Distribution des scores de classification couleur');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit deux modes, probablement les deux classes. Pour en être sûr :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "ax = sc[sc.color== 'white']['score'].hist(bins=25, figsize=(8,4), label='white', color='beige', alpha=0.5)\n",
    "sc[sc.color == 'red']['score'].hist(bins=25, ax=ax, label='red', color = 'mediumvioletred', alpha=0.5)\n",
    "ax.set_title(\"Distribution des scores pour les deux classes\")\n",
    "ax.plot([1, 1], [0, 100], color='green', ls='--', label=\"frontière ?\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a quelques confusions autour de 0 mais le modèle est pertinent au sens où la frontière entre les deux classes est assez nette : les deux cloches ne se superposent pas. Voyons avec les probabilités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_1 = reglog.predict_proba(X_test)[:, 1]\n",
    "pr = pd.DataFrame(proba_1, columns=['proba'])\n",
    "pr['color'] = y_test.values\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "pr[pr.color == 'white']['proba'].hist(bins=25, label='white', color = 'beige', alpha=0.5, ax=ax[0])\n",
    "pr[pr.color == 'red']['proba'].hist(bins=25, label='red', color = 'mediumvioletred', alpha=0.5, ax=ax[0])\n",
    "ax[0].set_title('Distribution des probabilités des deux classes')\n",
    "ax[0].plot([0.5, 0.5], [0, 1000], 'g--', label=\"frontière ?\")\n",
    "ax[0].legend();\n",
    "\n",
    "#l'échelle logarithmique permet de mieux voir les probabilités qui sont faibles\n",
    "pr[pr.color == 'white']['proba'].hist(bins=25, label='white', color = 'beige', alpha=0.5, ax=ax[1])\n",
    "pr[pr.color == 'red']['proba'].hist(bins=25, label='red', color = 'mediumvioletred', alpha=0.5, ax=ax[1])\n",
    "ax[1].plot([0.5, 0.5], [0, 1000], 'g--', label=\"frontière ?\")\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_title('Distribution des probabilités des deux classes\\néchelle logarithmique')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus l'aire commune aux deux distributions est petite, plus le modèle est confiant. Cette aire commune est reliée à la courbe [ROC](https://fr.wikipedia.org/wiki/Courbe_ROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "proba = reglog.predict_proba(X_test)\n",
    "fpr0, tpr0, thresholds0 = roc_curve(y_test, proba[:, 0], pos_label=reglog.classes_[0], drop_intermediate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*fpr* désigne le False Positive Rate autrement dit le taux de false positive. Si la tâche est de déterminer si un vin est blanc, le taux désigne la proportion de vins rouges classés parmi les vins blancs. C'est l'erreur de classification.\n",
    "\n",
    "*tpr* désigne le True Positive Rate c'est-à-dire le taux de True Positive.\n",
    "\n",
    "J'ai jamais été complètement au clair sur ce que représente chacune de ces informations mais l'avantage c'est qu'on trouve toujours toutes les infos dont on a besoin le moment venu. Par exemple, [ici](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = pd.DataFrame(dict(fpr=fpr0, tpr=tpr0, threshold=thresholds0)).copy()\n",
    "tp.drop(0, axis=0, inplace=True) #suppression du 1er seuil fixé arbitrairement à 2\n",
    "tp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = tp.plot(x=\"threshold\", y=['fpr', 'tpr'], figsize=(6,6))\n",
    "ax.set_title(\"Evolution de FPR, TPR\\nen fonction du seuil au delà duquel\\n\" + \n",
    "             \"la réponse du classifieur est validée\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "# aucf = roc_auc_score(y_test == clr.classes_[0], probas[:, 0]) # première méthode\n",
    "aucf = auc(fpr0, tpr0)  # seconde méthode\n",
    "ax.plot(fpr0, tpr0, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur couleur des vins')\n",
    "ax.text(0.5, 0.3, \"plus mauvais que\\nle hasard dans\\ncette zone\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mesure [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) ou Area Under the Curve est l'aire sous la courbe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux autres métriques sont très utilisées, la [précision](https://en.wikipedia.org/wiki/Precision_and_recall) et le [rappel](https://en.wikipedia.org/wiki/Precision_and_recall). Pour chaque classifieur, on peut déterminer un seuil *s* au delà duquel la réponse est validée avec une bonne confiance. Parmi toutes les réponses validées, la précision est le nombre de réponses correctes rapporté au nombre de réponses validées, le rappel est le nombre de réponses correctes rapportées à toutes qui aurait dû être validées. On calcule aussi la métrique *F1* qui est une sorte de moyenne entre les deux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probas[:, 0], pos_label=reglog.classes_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = pd.DataFrame(dict(precision=precision, recall=recall, \n",
    "                             threshold=[0] + list(thresholds)))\n",
    "pr['F1']= 2 * (pr.precision * pr.recall) / (pr.precision + pr.recall)\n",
    "pr.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pr.plot(x=\"threshold\", y=['precision', 'recall', 'F1'], figsize=(6,6))\n",
    "ax.set_title(\"Evolution de la précision et du rappel\\nen fonction du seuil au delà duquel\\n\" + \n",
    "             \"la réponse du classifieur est validée\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
