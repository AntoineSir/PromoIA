{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AZML III Pipeline - hyper parametre et Auto ml\n",
    "Laurent Cetinsoy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML\n",
    "\n",
    "Le machine learning, c'est bien. Le terme buzz du moment c'est AutoML : automatiser la recherche du meilleur modèle ou des meilleurs hyper paramètres pour un dataset donné. Plein d'entreprises proposent des outils pour le faire et microsoft ne déroge pas à la règle : le sdk permet d'en faire.  \n",
    "\n",
    "Reférence utile :\n",
    "\n",
    "- https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train\n",
    "- https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml\n",
    "\n",
    "Les concepts / classes utiles sont : AutoMLConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger le Workspace comme d'hab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='azml1_lk',\n",
    "                      subscription_id='50f55159-0c1b-4154-9452-e079e6b5e663',\n",
    "                      resource_group='AZML',\n",
    "                      location = 'northeurope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config(\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "On va entraîner un modèle de régression sur le dataset house. Charger le dataset house.csv avec pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>nb_rooms</th>\n",
       "      <th>garden</th>\n",
       "      <th>orientation</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197.330478</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sud</td>\n",
       "      <td>5.156551e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185.361036</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sud</td>\n",
       "      <td>9.341416e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.113286</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Est</td>\n",
       "      <td>6.894269e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139.920144</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Ouest</td>\n",
       "      <td>4.307298e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167.917466</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Nord</td>\n",
       "      <td>1.267890e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         size  nb_rooms  garden orientation         price\n",
       "0  197.330478         1       0         Sud  5.156551e+05\n",
       "1  185.361036         2       0         Sud  9.341416e+05\n",
       "2  195.113286         3       0         Est  6.894269e+06\n",
       "3  139.920144         3       1       Ouest  4.307298e+06\n",
       "4  167.917466         1       0        Nord  1.267890e+06"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"house.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On prend un autre dataset car résultats pas très cohérents\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "df = pd.read_csv(load_boston().filename, skiprows=0, header=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire un train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 14), (102, 14))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=64)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un objet de la classe AutoMLConfig. Configurer l'experience pour que ça ne dure pas trop longtemps  avec notamment les paramètre experiment_timeout_minute et n_cross_validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 2,\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    #\"n_cross_validations\": 5,\n",
    "    \"featurization\": 'auto'\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='regression',\n",
    "                             primary_metric='r2_score',\n",
    "                             training_data = X_train,\n",
    "                             validation_data = X_test,\n",
    "                             label_column_name=\"MEDV\",\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une experience \"myautoml_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "myautoml_reg = Experiment(ws, \"boston_house_regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancer votre experience avec le submit sur l'objet de config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n",
      "Parent Run ID: AutoML_04299c0b-70b6-412d-9870-31ad10dd4f59\n",
      "\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturization. Beginning to fit featurizers and featurize the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed fit featurizers and featurizing the dataset.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing feature values imputation\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  No feature missing values were detected in the training data.\n",
      "              Learn more about missing value imputation: https://aka.ms/AutomatedMLFeaturization\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "              Learn more about high cardinality feature handling: https://aka.ms/AutomatedMLFeaturization\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   MaxAbsScaler LightGBM                          0:00:17       0.8698    0.8698\n",
      "         1   MaxAbsScaler XGBoostRegressor                  0:00:17       0.8479    0.8698\n",
      "         2   StandardScalerWrapper XGBoostRegressor         0:00:16      -1.0000    0.8698\n",
      "         3   MaxAbsScaler LightGBM                          0:00:32       0.8104    0.8698\n",
      "         4   StandardScalerWrapper XGBoostRegressor         0:00:19       0.8599    0.8698\n",
      "         5   MaxAbsScaler RandomForest                      0:00:16       0.8136    0.8698\n",
      "         6   StandardScalerWrapper ElasticNet               0:00:19       0.7187    0.8698\n",
      "         7   MaxAbsScaler ExtremeRandomTrees                0:00:47       0.8515    0.8698\n",
      "         8   MaxAbsScaler LightGBM                          0:00:47       0.7468    0.8698\n",
      "         9   MaxAbsScaler ExtremeRandomTrees                0:00:16       0.8535    0.8698\n",
      "        10   MaxAbsScaler RandomForest                      0:00:13       0.7986    0.8698\n",
      "        11   MaxAbsScaler ElasticNet                        0:00:13       0.7185    0.8698\n",
      "        12   MaxAbsScaler ExtremeRandomTrees                0:00:19       0.8101    0.8698\n",
      "        13   MaxAbsScaler ExtremeRandomTrees                0:00:22       0.8175    0.8698\n",
      "        14   MaxAbsScaler RandomForest                      0:00:27       0.8328    0.8698\n",
      "        15   StandardScalerWrapper ExtremeRandomTrees       0:00:15       0.8307    0.8698\n",
      "        16   MaxAbsScaler RandomForest                      0:00:14       0.8084    0.8698\n",
      "        17   StandardScalerWrapper XGBoostRegressor         0:00:13      -1.0000    0.8698\n",
      "        18   StandardScalerWrapper LightGBM                 0:00:14       0.8507    0.8698\n",
      "        19   StandardScalerWrapper LightGBM                 0:00:11       0.8533    0.8698\n",
      "        20   MaxAbsScaler ElasticNet                        0:00:18       0.7182    0.8698\n",
      "        21   StandardScalerWrapper XGBoostRegressor         0:00:22       0.8553    0.8698\n",
      "        22   MaxAbsScaler ExtremeRandomTrees                0:00:15       0.6121    0.8698\n",
      "        23   MaxAbsScaler SGD                               0:00:14       0.6541    0.8698\n",
      "        24   MaxAbsScaler DecisionTree                      0:00:14       0.6994    0.8698\n",
      "        25   MaxAbsScaler LightGBM                          0:00:19      -0.0025    0.8698\n",
      "        26   MaxAbsScaler LightGBM                          0:00:49       0.7695    0.8698\n",
      "        27   MaxAbsScaler RandomForest                      0:00:10       0.8001    0.8698\n",
      "        28   StandardScalerWrapper ExtremeRandomTrees       0:00:15       0.6304    0.8698\n",
      "        29   StandardScalerWrapper XGBoostRegressor         0:00:13       0.8761    0.8761\n",
      "        30   TruncatedSVDWrapper XGBoostRegressor           0:00:14       0.6959    0.8761\n",
      "        31   StandardScalerWrapper DecisionTree             0:00:15       0.6945    0.8761\n",
      "        32   MaxAbsScaler GradientBoosting                  0:00:17       0.7630    0.8761\n",
      "        33   StandardScalerWrapper RandomForest             0:00:13       0.8295    0.8761\n",
      "        34   MaxAbsScaler ElasticNet                        0:00:13       0.7140    0.8761\n",
      "        35   SparseNormalizer XGBoostRegressor              0:00:13       0.8183    0.8761\n",
      "        36   SparseNormalizer RandomForest                  0:00:14       0.7455    0.8761\n",
      "        37   MaxAbsScaler ExtremeRandomTrees                0:00:15       0.6357    0.8761\n",
      "        38   MaxAbsScaler ExtremeRandomTrees                0:00:14       0.6578    0.8761\n",
      "        39   StandardScalerWrapper RandomForest             0:00:16       0.8246    0.8761\n",
      "        40   StandardScalerWrapper XGBoostRegressor         0:00:17       0.2116    0.8761\n",
      "        41   MaxAbsScaler RandomForest                      0:00:14       0.8405    0.8761\n",
      "        42   StandardScalerWrapper ElasticNet               0:00:13       0.7187    0.8761\n",
      "        43   MaxAbsScaler DecisionTree                      0:00:16       0.7476    0.8761\n",
      "        44   MaxAbsScaler RandomForest                      0:00:12       0.7804    0.8761\n",
      "        45   StandardScalerWrapper XGBoostRegressor         0:00:16       0.8412    0.8761\n",
      "        46   MaxAbsScaler RandomForest                      0:00:13       0.7538    0.8761\n",
      "        47   MaxAbsScaler RandomForest                      0:00:13       0.7984    0.8761\n",
      "        48   MaxAbsScaler DecisionTree                      0:00:13       0.7284    0.8761\n",
      "        49   StandardScalerWrapper ElasticNet               0:00:09       0.6622    0.8761\n",
      "        50   TruncatedSVDWrapper XGBoostRegressor           0:00:18      -1.0000    0.8761\n",
      "        51   MaxAbsScaler RandomForest                      0:00:16       0.7362    0.8761\n",
      "        52   MaxAbsScaler RandomForest                      0:00:14       0.8398    0.8761\n",
      "        53   MaxAbsScaler DecisionTree                      0:00:14       0.7526    0.8761\n",
      "        54   MaxAbsScaler RandomForest                      0:00:18       0.8478    0.8761\n",
      "        55   StandardScalerWrapper XGBoostRegressor         0:00:16       0.8543    0.8761\n",
      "        56   MaxAbsScaler RandomForest                      0:00:14       0.8092    0.8761\n",
      "        57   MaxAbsScaler RandomForest                      0:00:15       0.7468    0.8761\n",
      "        58   MaxAbsScaler RandomForest                      0:00:20       0.7737    0.8761\n",
      "        59   MaxAbsScaler DecisionTree                      0:00:14       0.7424    0.8761\n",
      "        60   StandardScalerWrapper XGBoostRegressor         0:00:10       0.8704    0.8761\n",
      "        61   MaxAbsScaler RandomForest                      0:00:18       0.8320    0.8761\n",
      "        62   MaxAbsScaler RandomForest                      0:00:19       0.8125    0.8761\n",
      "        63   MaxAbsScaler RandomForest                      0:00:26       0.8227    0.8761\n",
      "        64   MaxAbsScaler RandomForest                      0:00:22       0.7594    0.8761\n",
      "        65   StandardScalerWrapper XGBoostRegressor         0:00:31      -1.0000    0.8761\n",
      "        66                                                  0:02:13          nan    0.8761\n",
      "ERROR:                                                 \n",
      "        67   StackEnsemble                                  0:01:46       0.8749    0.8761\n",
      "Stopping criteria reached at iteration 68. Ending experiment.\n",
      "****************************************************************************************************\n",
      "Current status: BestRunExplainModel. Best run model explanations started\n",
      "Current status: ModelExplanationDataSetSetup. Model explanations data setup completed\n",
      "Current status: PickSurrogateModel. Choosing LightGBM as the surrogate model for explanations\n",
      "Current status: EngineeredFeatureExplanations. Computation of engineered features started\n",
      "Current status: EngineeredFeatureExplanations. Computation of engineered features completed\n",
      "Current status: BestRunExplainModel. Best run model explanations completed\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "local_run = myautoml_reg.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: boston_house_regression,\n",
      "Id: AutoML_04299c0b-70b6-412d-9870-31ad10dd4f59_29,\n",
      "Type: None,\n",
      "Status: Completed)\n",
      "RegressionPipeline(pipeline=Pipeline(memory=None,\n",
      "     steps=[('datatransformer', DataTransformer(enable_dnn=None, enable_feature_sweeping=None,\n",
      "        feature_sweeping_config=None, feature_sweeping_timeout=None,\n",
      "        featurization_config=None, force_text_dnn=None,\n",
      "        is_cross_validation=None, is_onnx_compatible=None, logger=None,\n",
      "        obser...      subsample=0.6, tree_method='auto', validate_parameters=1,\n",
      "         verbose=-10, verbosity=0))]),\n",
      "          stddev=None)\n"
     ]
    }
   ],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ça fail, ce n'est pas très grave. Essayez de débug mais ne perdez pas trop de temps. Automl a besoin de bcp de librairies et le faire marcher en local est galère. Passer plutôt en remote ou lancer un notebook sur votre workspace azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option en remote\n",
    "\n",
    "Pour cela on va devoir utiliser une ressource en remote. \n",
    "Récupérer une référence au AmlCompute du notebook précédent si vous l'avez fait.\n",
    "Sinon en créér un grâce à la méthode provisioning_configuration de la classe AmlCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une instance de la classe ComputeTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget\n",
    "cpu_cluster = ComputeTarget.create(ws, \"elkalkul\", compute_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un CT plus puissante pour aller plus vite\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS4_V2', max_nodes=8)\n",
    "cpu_cluster = ComputeTarget.create(ws, \"elkalkul-big\", compute_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload du dataset\n",
    "\n",
    "en remote on est obligé d'upload notre dataset sur un DataStore et récupérer une référence via un `Dataset`. En effet on ne peut pas envoyer tel quel un df pandas en mémoire vers la compute target distante.\n",
    "\n",
    "Uploader et récupérer une référence du fichier houses_automl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Target already exists. Skipping upload for house.csv\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_workspaceblobstore"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload_files(['./house.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on réécrit un csv avec la première ligne en moins pour pouvoir ensuite le charger sur le datastore\n",
    "df.to_csv('boston_house.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Target already exists. Skipping upload for boston_house.csv\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_workspaceblobstore"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload_files(['./boston_house.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "source = [(ds, 'boston_house.csv')]\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test, data_train = dataset.random_split(0.2, seed=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une AutoMLConfig qui utilise le dataset uploadé et la compute target à distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 2,\n",
    "    \"experiment_timeout_hours\": 0.3,\n",
    "    \"featurization\": 'auto',\n",
    "    \"max_concurrent_iterations\": 4#,\n",
    "    #\"n_cross_validations\": 5\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='regression',\n",
    "                             primary_metric='r2_score',\n",
    "                             compute_target=cpu_cluster,\n",
    "                             training_data = data_train,\n",
    "                             validation_data = data_test,\n",
    "                             label_column_name=\"MEDV\",\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une experience \"remoteautoml\". Submit l'experience avec l'objet automl config que vous venez de créer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote or ADB.\n",
      "Running on remote compute: elkalkul\n",
      "Parent Run ID: AutoML_4534478e-1ddc-42ba-9cb6-0cce7d7ce39a\n",
      "\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing feature values imputation\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  No feature missing values were detected in the training data.\n",
      "              Learn more about missing value imputation: https://aka.ms/AutomatedMLFeaturization\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "              Learn more about high cardinality feature handling: https://aka.ms/AutomatedMLFeaturization\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   MaxAbsScaler LightGBM                          0:00:28       0.8451    0.8451\n",
      "         3   MaxAbsScaler LightGBM                          0:00:54       0.7928    0.8451\n",
      "         1   MaxAbsScaler XGBoostRegressor                  0:00:32       0.8539    0.8539\n",
      "         2   StandardScalerWrapper XGBoostRegressor         0:00:32       0.8287    0.8539\n",
      "         4   StandardScalerWrapper XGBoostRegressor         0:00:33       0.8443    0.8539\n",
      "         8   MaxAbsScaler ExtremeRandomTrees                0:00:32       0.7832    0.8539\n",
      "         9   MaxAbsScaler LightGBM                          0:00:24       0.6996    0.8539\n",
      "        10   MinMaxScaler ExtremeRandomTrees                0:00:24       0.8148    0.8539\n",
      "         6   StandardScalerWrapper ElasticNet               0:03:07       0.6700    0.8539\n",
      "        11   MaxAbsScaler RandomForest                      0:00:23       0.7556    0.8539\n",
      "         7   RobustScaler ElasticNet                        0:03:15       0.6487    0.8539\n",
      "         5   MinMaxScaler RandomForest                      0:03:16       0.7895    0.8539\n",
      "        13   MinMaxScaler ExtremeRandomTrees                0:00:28       0.7284    0.8539\n",
      "        12   MaxAbsScaler ElasticNet                        0:00:25       0.6692    0.8539\n",
      "        14   MaxAbsScaler ExtremeRandomTrees                0:00:29       0.7844    0.8539\n",
      "        15   MinMaxScaler RandomForest                      0:00:28       0.8133    0.8539\n",
      "        17   StandardScalerWrapper ExtremeRandomTrees       0:00:31       0.8119    0.8539\n",
      "        16   RobustScaler RandomForest                      0:00:33       0.7773    0.8539\n",
      "        18   MinMaxScaler RandomForest                      0:00:29       0.8280    0.8539\n",
      "        19   StandardScalerWrapper ElasticNet               0:00:26       0.6620    0.8539\n",
      "        20   RobustScaler RandomForest                      0:00:32       0.6693    0.8539\n",
      "        21   StandardScalerWrapper XGBoostRegressor         0:00:27       0.8622    0.8622\n",
      "        23   MaxAbsScaler ElasticNet                        0:00:26       0.6683    0.8622\n",
      "        22   StandardScalerWrapper LightGBM                 0:00:40       0.8608    0.8622\n",
      "        24   StandardScalerWrapper XGBoostRegressor         0:00:23       0.8482    0.8622\n",
      "        27   MinMaxScaler LightGBM                          0:00:32       0.8385    0.8622\n",
      "        26   RobustScaler SGD                               0:00:31       0.6300    0.8622\n",
      "        25   PCA XGBoostRegressor                           0:00:29      -0.5682    0.8622\n",
      "        28   MinMaxScaler LightGBM                          0:00:28       0.8264    0.8622\n",
      "        29   PCA XGBoostRegressor                           0:00:24      -0.3953    0.8622\n",
      "        31   StandardScalerWrapper DecisionTree             0:00:30       0.7766    0.8622\n",
      "        30   StandardScalerWrapper DecisionTree             0:00:32       0.8099    0.8622\n",
      "        32   StandardScalerWrapper DecisionTree             0:00:27       0.6935    0.8622\n",
      "        33   StandardScalerWrapper DecisionTree             0:00:28       0.5583    0.8622\n",
      "        35   RobustScaler ExtremeRandomTrees                0:00:26       0.7560    0.8622\n",
      "        34   StandardScalerWrapper XGBoostRegressor         0:00:27       0.8572    0.8622\n",
      "        36   StandardScalerWrapper XGBoostRegressor         0:00:26       0.8805    0.8805\n",
      "        37   StandardScalerWrapper RandomForest             0:00:27       0.7475    0.8805\n",
      "        39   MaxAbsScaler LightGBM                          0:00:24       0.8327    0.8805\n",
      "        38   MinMaxScaler LightGBM                          0:00:30       0.8493    0.8805\n",
      "        40   SparseNormalizer XGBoostRegressor              0:00:30       0.7388    0.8805\n",
      "        41   StandardScalerWrapper RandomForest             0:00:30       0.8499    0.8805\n",
      "        42   MaxAbsScaler RandomForest                      0:00:27       0.8320    0.8805\n",
      "        44   StandardScalerWrapper ExtremeRandomTrees       0:00:30       0.7798    0.8805\n",
      "        43   StandardScalerWrapper ExtremeRandomTrees       0:00:28       0.7772    0.8805\n",
      "        45   StandardScalerWrapper XGBoostRegressor         0:00:32       0.8132    0.8805\n",
      "        46   MaxAbsScaler RandomForest                      0:00:29       0.8667    0.8805\n",
      "        47   StandardScalerWrapper LightGBM                 0:00:31       0.8610    0.8805\n",
      "        48   StandardScalerWrapper LightGBM                 0:00:25       0.8867    0.8867\n",
      "        50   MaxAbsScaler DecisionTree                      0:00:32       0.7462    0.8867\n",
      "        49   StandardScalerWrapper XGBoostRegressor         0:00:26       0.8468    0.8867\n",
      "        51   StandardScalerWrapper LightGBM                 0:00:27       0.8624    0.8867\n",
      "        52   RobustScaler LightGBM                          0:00:32       0.8814    0.8867\n",
      "        54   RobustScaler ExtremeRandomTrees                0:00:21       0.7701    0.8867\n",
      "        53   MinMaxScaler LightGBM                          0:00:22       0.7771    0.8867\n",
      "        55   MaxAbsScaler LightGBM                          0:00:22       0.8354    0.8867\n",
      "        56   MaxAbsScaler LightGBM                          0:00:29       0.8033    0.8867\n",
      "        58   MaxAbsScaler LightGBM                          0:00:23       0.8433    0.8867\n",
      "        59   RobustScaler ExtremeRandomTrees                0:00:28       0.7666    0.8867\n",
      "        57   MinMaxScaler LightGBM                          0:00:34       0.8150    0.8867\n",
      "        60   StandardScalerWrapper LightGBM                 0:00:24       0.7648    0.8867\n",
      "        61   StandardScalerWrapper DecisionTree             0:00:26       0.8228    0.8867\n",
      "        62   StandardScalerWrapper RandomForest             0:00:30       0.8631    0.8867\n",
      "        63   RobustScaler RandomForest                      0:00:33       0.7917    0.8867\n",
      "        66                                                  0:00:07          nan    0.8867\n",
      "        65   StandardScalerWrapper LightGBM                 0:00:24          nan    0.8867\n",
      "        64   RobustScaler LightGBM                          0:00:24          nan    0.8867\n",
      "        68    StackEnsemble                                 0:00:48       0.8881    0.8881\n",
      "        67    VotingEnsemble                                0:00:49       0.8968    0.8968\n"
     ]
    }
   ],
   "source": [
    "remoteautoml = Experiment(ws, \"boston_house_regression_remote\")\n",
    "remote_run = remoteautoml.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "best_run, fitted_model = remote_run.get_output()\n",
    "```\n",
    "__/!\\ /!\\ /!\\__ Problème avec la sérialisation du XGboost, pour simplifier on peut éventuellement mettre XGBoost et LightGBM dans le paramètre blocked_models pour avoir que des moèles de sklearn de sorte qu'il y ait pas ce souci. Sinon il faut fouiller pour le résoudre (la difficulté étant que c'est autoML qui s'occupe d'enregistrer/sérialiser les modèles donc il s'agit de trouver comment avoir la main là-dessus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que cela a bien marché et récupérer le meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper drive \n",
    "\n",
    "Kezako ? Module d'azure machine learning qui permet de faire de la recherche d'hyper parametres.\n",
    "\n",
    "Reférences utiles : \n",
    "- https://docs.microsoft.com/fr-fr/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py\n",
    "- https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters\n",
    "\n",
    "\n",
    "Les classes concepts intéressant sont HyperParameterSampling, HyperDriveRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle différence entre hyperdrive et automl ? C'est pas la même chose ? Pourquoi deux choses différentes ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automl teste plusieurs modèles, hyperdrive teste plusieurs hyperparamètres d'un même modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quoi sert la classe HyperParameterSampling ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permet de spécifier comment on \"chercher\" des hyperparametres (cf https://docs.microsoft.com/fr-fr/python/api/azureml-train-core/azureml.train.hyperdrive.hyperparametersampling?view=azure-ml-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles sont les principales classes filles ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Cette classe encapsule l’espace hyperparamétrique, la méthode d’échantillonnage et les propriétés supplémentaires pour les classes d’échantillonnage dérivées : BayesianParameterSampling , GridParameterSampling et RandomParameterSampling .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles semblent-être les méthodes les plus importantes ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quoi sert la classe HyperDriveRun ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classe fille de la classe run, c'est une classe particulière qui permet de récupérer les résultats de la recherche d'hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va faire la recherche d'hyperparamètre sur notre modèle de régression de tout à l'heure (house).\n",
    "Créer un dictionnaire avec les noms des paramètres que vous voulez recherche (par exemple le learning rate ou le taux de régularisation), cf https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#define-search-space\n",
    "\n",
    "Les faire commencer par un --. Par exemple --n_epochs et pas n_epochs. Cela servira pour passer les arguments à notre script d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.parameter_expressions import choice, uniform\n",
    "param_dict = {\n",
    "    \"--loss\": choice('ls', 'lad', 'huber'),\n",
    "    \"--learning_rate\": uniform(0.05, 0.1),\n",
    "    \"--n_estimators\": choice(50, 100, 250, 500, 1000),\n",
    "    \"--criterion\": choice('friedman_mse', 'mse', 'mae'),\n",
    "    \"--max_depth\": choice(3, 5, 7)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instancier un objet de la classe RandomParameterSampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "param_sampling = RandomParameterSampling(param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spécifier la métrique que vous voulez optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maximiser le R2 qu'on déclarera dans les paramètres de HyperDriveConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choisir un critère d'arrêt de la recherche d'hyper paramètre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editer le fichier d'entraînement house_training.py pour qu'il soit compatible avec l'entraînement. Il faut utiliser le module argpass pour récupérer les paramètres dont vous voulez faire la recherche. Cf https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! Editer le fichier !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un estimateur SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.sklearn import SKLearn\n",
    "estimator = SKLearn(source_directory=\".\", \n",
    "                    compute_target=cpu_cluster,\n",
    "                    entry_script='train_house.py'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une configuration HyperdriveConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(estimator=estimator,\n",
    "                          hyperparameter_sampling=param_sampling, \n",
    "                          policy=early_termination_policy,\n",
    "                          primary_metric_name=\"r2_score\", \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                          max_total_runs=100,\n",
    "                          max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumettre l'experience localement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"hyper_param_tuning\")\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et récupérer le meilleur set d'hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--learning_rate', '0.06967842644937153', '--loss', 'ls', '--n_estimators', '500']\n",
      "{'r2_score': 0.8950570510468429, 'Loss function': 'ls', 'Learning Rate': 0.06967842644937153, 'Number of Estimators': 500, 'MSE': 10.228252775530551}\n"
     ]
    }
   ],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "\n",
    "print(parameter_values)\n",
    "print(best_run_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
