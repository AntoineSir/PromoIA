{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recurrent Neural Network - Analyse de sentiments**\n",
    "\n",
    "Le but de cet exercice est la classification binaire (positive ou négative) de reviews en utilisant un RNN sur un dataset de commentaires IMDB que vous trouverez dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des librairies\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import des données et preprocessing**\n",
    "\n",
    "1. Importer les 2 fichiers \".txt\" de data/imdb\n",
    "2. Vérifier ce qu'il y a dedans si c'est pas déjà fait...\n",
    "3. Un peu de preprocessing de texte, allez on va chercher dans sa petite mémoire et sinon dans son gros ordinateur :\n",
    ">- convertir en minuscules\n",
    ">- retirer la ponctuation\n",
    ">- créer une liste des reviews et déterminer combien il y en a. Pareil pour les labels. Inch'Allah y en aura autant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lire les \".txt\"\n",
    "with open('data/imdb/reviews.txt', 'r') as f:\n",
    "    reviews_str = f.read()\n",
    "with open('data/imdb/labels.txt', 'r') as f:\n",
    "    labels_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 33678267,\n",
       " 'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews\n",
    "type(reviews_str), len(reviews_str), reviews_str[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 225000,\n",
       " 'positive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nn')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "type(labels_str), len(labels_str), labels_str[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en minuscules\n",
    "reviews_str = reviews_str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sans ponctuation\n",
    "reviews_str = reviews_str.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split et nb de review\n",
    "reviews = reviews_str.split('\\n')\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split et nb de label\n",
    "labels = labels_str.split('\\n')\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Tokenisation ou encodage**\n",
    "\n",
    "Le but est de remplacer les mots des reviews par des entiers. Si vous voulez faire à votre sauce, vous y êtes encouragés !\n",
    "\n",
    "Sinon, les quelques étapes décrites ci-dessous vous permettront de le faire :\n",
    ">- compter l'ensemble des mots (vous pouvez utiliser `Counter` de la librairie `collections` qui est une des plus rapide dans ce domaine)\n",
    ">- les trier par ordre décroissant d'occurrences\n",
    ">- créer un dictionnaire `word_mapping` où les clés sont les mots et les valeurs l'entier associé (votre dico doit être `{'the': 1, 'and': 2, 'a': 3, 'of': 4,..., 'muppified': 74070, 'whelk': 74071, 'hued': 74072}}`). C'est volontaire que ça commence à 1 car on utilisera le \"0\" comme caractère pour remplir les reviews les plus courtes afin qu'elles aient toutes la même taille...\n",
    ">- encoder les mots (c'est-à-dire les remplacer par l'entier qui les représente)\n",
    ">- encoder les labels (ça c'est fastoche, on se débrouille)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comptage des mots et tri par occurrence\n",
    "words = ' '.join(reviews).split()\n",
    "word_count = Counter(words)\n",
    "sorted_words = sorted(word_count, key=word_count.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire de correspondance mot = entier\n",
    "word_mapping = {w:i+1 for i,w in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23], [63, 4, 3, 125, 36, 47, 7472, 1395, 16, 3, 4181, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10624, 7373, 300, 6, 667, 83, 35, 2116, 1086, 2989, 34, 1, 898, 46417, 4, 8, 13, 5096, 464, 8, 2656, 1721, 1, 221, 57, 17, 58, 794, 1297, 832, 228, 8, 43, 98, 123, 1469, 59, 147, 38, 1, 963, 142, 29, 667, 123, 1, 13584, 410, 61, 94, 1774, 306, 755, 5, 3, 819, 10396, 22, 3, 1724, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 617, 34, 682, 85, 28785, 28786, 682, 374, 3341, 11398, 2, 16371, 7946, 51, 29, 108, 3324]]\n"
     ]
    }
   ],
   "source": [
    "# encodage des mots\n",
    "reviews_num = [list(map(lambda w: word_mapping[w], rvw.split())) for rvw in reviews]\n",
    "print(reviews_num[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodage des labels\n",
    "labels_num = [1*(label == 'positive') for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Longueur des séquences**\n",
    "\n",
    "Il s'agit ici d'analyser la longueur des reviews pour déterminer éventuellement des outliers et choisir ce qu'on en fait. On va aussi \"uniformiser\" la longueur des séquence.\n",
    "\n",
    "1. Avec la méthode de votre choix (graphique, stats desc...), étudier la longueur des reviews\n",
    "2. Déterminer s'il y a des outliers et ce que vous souhaitez en faire (si vous les décidez de les supprimer, attention de bien supprimer aussi les labels correspondants...)\n",
    "3. Ça a été évoqué un peu plus haut, on veut que nos reviews aient toutes la même taille pour faciliter l'entraînement du réseau. Par conséquent on va ajouter des 0 aux reviews les plus courtes et tronquer les reviews les plus longues (*padding/truncating*, si vous vous souvenez bien on a vu il y a peu le *zero-padding* dans un certain cas...bon ben c'est pareil).  \n",
    ">- définir une fonction `trunc_pad(review_list, length)` :\n",
    ">>- qui prend en paramètres la liste des reviews (chaque review étant encodée en une liste d'entiers) et une longueur donnée\n",
    ">>- et qui retourne un array 2D avec (en ligne) les reviews trop longues tronquées et des 0 à gauche pour les reviews trop courtes. Avant de vous lancer et pour vous assurer d'avoir compris, quelles dimensions doit avoir votre array en sortie ?\n",
    ">>- tester votre fonction avec une longueur fixée à 250 et afficher les 5 premières valeurs des 5 premières reviews. Vous devez obtenir ça:  \n",
    "\\[[    0     0     0     0     0]  \n",
    "[    0     0     0     0     0]  \n",
    "[22382    42 46418    15   706]  \n",
    "[ 4505   505    15     3  3342]  \n",
    "[    0     0     0     0     0]\\]\n",
    ">- maintenant, que c'est fait, je peux vous le dire, il y a une fonction dans `keras.preprocessing` qui permet de le faire. La trouver et comparer les temps d'éxecution des 2 fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25001.000000\n",
       "mean       240.798208\n",
       "std        179.020628\n",
       "min          0.000000\n",
       "25%        130.000000\n",
       "50%        179.000000\n",
       "75%        293.000000\n",
       "max       2514.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAX0UlEQVR4nO3db7Bc9X3f8fenIiYyNg5/yq0qaSo5VtLyJ2nMLVXq1nNbmqDYGYvOmBl5cFFaZjRliOO0eFJRP3CeaAbSYDfQwowaKMJVwSpxKk09pGZwdjyd4Y+FY1sIonAdKFyjoFA7DtetMSLfPtifmvXVXl1pV/eurvb9mtnZs99zfnvOdxfuR+ec3T2pKiRJ+iuj3gBJ0pnBQJAkAQaCJKkxECRJgIEgSWrOGfUGDOriiy+udevWDTT2e9/7Huedd97p3aAz3Lj1PG79wvj1PG79wunp+emnn36tqv5qv3nLNhDWrVvH/v37Bxrb6XSYmpo6vRt0hhu3nsetXxi/nsetXzg9PSf5X/PN85CRJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCVjG31QexoFvfZdf2v6Fkaz7xds+OJL1StJC3EOQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAScRCEnuS3IkyTNz6h9LcijJwSS/0VO/Ncl0m3dNT/3KJAfavDuTpNXPTfK5Vn8yybrT2J8k6SSdzB7C/cCm3kKSfwhsBn6qqi4DfrPVLwW2AJe1MXcnWdGG3QNsAza027HnvBH4TlW9B/gMcPsQ/UiSBrRgIFTVl4FvzynfBNxWVW+0ZY60+mbgoap6o6peAKaBq5KsAs6vqserqoAHgGt7xuxq0w8DVx/be5AkLZ1Bf9zuJ4B/kGQH8H3gE1X1FWA18ETPcjOt9mabnlun3b8MUFVHk3wXuAh4be5Kk2yju5fBxMQEnU5noI2fWAm3XHF0oLHDGnSbhzU7OzuydY/CuPUL49fzuPULi9/zoIFwDnABsBH4O8CeJO8G+v3Lvk5QZ4F5P1ys2gnsBJicnKypqalT2+rmrt17uePAaH7o9cXrp0ay3k6nw6Cv13I0bv3C+PU8bv3C4vc86KeMZoDPV9dTwF8AF7f62p7l1gCvtPqaPnV6xyQ5B3gXxx+ikiQtskED4b8B/wggyU8Ab6N7iGcfsKV9cmg93ZPHT1XVYeD1JBvb+YEbgL3tufYBW9v0h4EvtfMMkqQltOBxkyQPAlPAxUlmgE8B9wH3tY+i/gDY2v6IH0yyB3gWOArcXFVvtae6ie4nllYCj7QbwL3AZ5NM090z2HJ6WpMknYoFA6GqPjLPrI/Os/wOYEef+n7g8j717wPXLbQdkqTF5TeVJUmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAScRCEnuS3KkXQxn7rxPJKkkF/fUbk0yneRQkmt66lcmOdDm3dmunEa7utrnWv3JJOtOU2+SpFNwMnsI9wOb5haTrAV+Dnipp3Yp3SueXdbG3J1kRZt9D7CN7mU1N/Q8543Ad6rqPcBngNsHaUSSNJwFA6Gqvkz/i95/Bvg1oPf6x5uBh6rqjap6AZgGrkqyCji/qh5vl9p8ALi2Z8yuNv0wcPWxvQdJ0tJZ8BKa/ST5EPCtqvr6nL/dq4Eneh7PtNqbbXpu/diYlwGq6miS7wIXAa/1We82unsZTExM0Ol0Btl8JlbCLVccHWjssAbd5mHNzs6ObN2jMG79wvj1PG79wuL3fMqBkOTtwCeBn+83u0+tTlA/0Zjji1U7gZ0Ak5OTNTU1tdDm9nXX7r3ccWCgLBzai9dPjWS9nU6HQV+v5Wjc+oXx63nc+oXF73mQTxn9OLAe+HqSF4E1wFeT/DW6//Jf27PsGuCVVl/Tp07vmCTnAO+i/yEqSdIiOuVAqKoDVXVJVa2rqnV0/6C/t6r+BNgHbGmfHFpP9+TxU1V1GHg9ycZ2fuAGYG97yn3A1jb9YeBL7TyDJGkJnczHTh8EHgd+MslMkhvnW7aqDgJ7gGeB3wNurqq32uybgN+me6L5m8AjrX4vcFGSaeBfAdsH7EWSNIQFD6RX1UcWmL9uzuMdwI4+y+0HLu9T/z5w3ULbIUlaXH5TWZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKak7li2n1JjiR5pqf2b5P8YZJvJPndJD/WM+/WJNNJDiW5pqd+ZZIDbd6d7VKatMttfq7Vn0yy7vS2KEk6GSezh3A/sGlO7VHg8qr6KeCPgFsBklwKbAEua2PuTrKijbkH2Eb3Ossbep7zRuA7VfUe4DPA7YM2I0ka3IKBUFVfBr49p/bFqjraHj4BrGnTm4GHquqNqnqB7vWTr0qyCji/qh6vqgIeAK7tGbOrTT8MXH1s70GStHQWvKbySfjnwOfa9Gq6AXHMTKu92abn1o+NeRmgqo4m+S5wEfDa3BUl2UZ3L4OJiQk6nc5AGzyxEm654ujCCy6CQbd5WLOzsyNb9yiMW78wfj2PW7+w+D0PFQhJPgkcBXYfK/VZrE5QP9GY44tVO4GdAJOTkzU1NXUqm/v/3bV7L3ccOB1ZeOpevH5qJOvtdDoM+notR+PWL4xfz+PWLyx+zwN/yijJVuAXgevbYSDo/st/bc9ia4BXWn1Nn/oPjUlyDvAu5hyikiQtvoECIckm4F8DH6qq/9Mzax+wpX1yaD3dk8dPVdVh4PUkG9v5gRuAvT1jtrbpDwNf6gkYSdISWfC4SZIHgSng4iQzwKfofqroXODRdv73iar6F1V1MMke4Fm6h5Jurqq32lPdRPcTSyuBR9oN4F7gs0mm6e4ZbDk9rUmSTsWCgVBVH+lTvvcEy+8AdvSp7wcu71P/PnDdQtshSVpcflNZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkScBKBkOS+JEeSPNNTuzDJo0meb/cX9My7Ncl0kkNJrumpX5nkQJt3Z7tyGu3qap9r9SeTrDvNPUqSTsLJ7CHcD2yaU9sOPFZVG4DH2mOSXEr3imeXtTF3J1nRxtwDbKN7Wc0NPc95I/CdqnoP8Bng9kGbkSQNbsFAqKovc/xF7zcDu9r0LuDanvpDVfVGVb0ATANXJVkFnF9Vj7frJT8wZ8yx53oYuPrY3oMkaekseAnNeUxU1WGAqjqc5JJWXw080bPcTKu92abn1o+Nebk919Ek3wUuAl6bu9Ik2+juZTAxMUGn0xls41fCLVccHWjssAbd5mHNzs6ObN2jMG79wvj1PG79wuL3PGggzKffv+zrBPUTjTm+WLUT2AkwOTlZU1NTA2wi3LV7L3ccON2tn5wXr58ayXo7nQ6Dvl7L0bj1C+PX87j1C4vf86CfMnq1HQai3R9p9Rlgbc9ya4BXWn1Nn/oPjUlyDvAujj9EJUlaZIMGwj5ga5veCuztqW9pnxxaT/fk8VPt8NLrSTa28wM3zBlz7Lk+DHypnWeQJC2hBY+bJHkQmAIuTjIDfAq4DdiT5EbgJeA6gKo6mGQP8CxwFLi5qt5qT3UT3U8srQQeaTeAe4HPJpmmu2ew5bR0Jkk6JQsGQlV9ZJ5ZV8+z/A5gR5/6fuDyPvXv0wJFkjQ6flNZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSMGQgJPmXSQ4meSbJg0l+NMmFSR5N8ny7v6Bn+VuTTCc5lOSanvqVSQ60eXe2q6pJkpbQwIGQZDXwK8BkVV0OrKB7tbPtwGNVtQF4rD0myaVt/mXAJuDuJCva090DbKN7yc0Nbb4kaQkNe8joHGBlknOAtwOvAJuBXW3+LuDaNr0ZeKiq3qiqF4Bp4Kokq4Dzq+rxdi3lB3rGSJKWyIKX0JxPVX0ryW/Svaby/wW+WFVfTDJRVYfbMoeTXNKGrAae6HmKmVZ7s03PrR8nyTa6exJMTEzQ6XQG2vaJlXDLFUcHGjusQbd5WLOzsyNb9yiMW78wfj2PW7+w+D0PHAjt3MBmYD3wZ8B/TfLREw3pU6sT1I8vVu0EdgJMTk7W1NTUKWzxX7pr917uODBw60N58fqpkay30+kw6Ou1HI1bvzB+PY9bv7D4PQ9zyOgfAy9U1Z9W1ZvA54G/B7zaDgPR7o+05WeAtT3j19A9xDTTpufWJUlLaJhAeAnYmOTt7VNBVwPPAfuArW2ZrcDeNr0P2JLk3CTr6Z48fqodXno9ycb2PDf0jJEkLZFhziE8meRh4KvAUeAP6B7OeQewJ8mNdEPjurb8wSR7gGfb8jdX1Vvt6W4C7gdWAo+0myRpCQ11IL2qPgV8ak75Dbp7C/2W3wHs6FPfD1w+zLZIkobjN5UlSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBQwZCkh9L8nCSP0zyXJKfTXJhkkeTPN/uL+hZ/tYk00kOJbmmp35lkgNt3p3tymmSpCU07B7CbwG/V1V/E/hpupfQ3A48VlUbgMfaY5JcCmwBLgM2AXcnWdGe5x5gG93Lam5o8yVJS2jgQEhyPvB+4F6AqvpBVf0ZsBnY1RbbBVzbpjcDD1XVG1X1AjANXJVkFXB+VT1eVQU80DNGkrREhrmE5ruBPwX+U5KfBp4GPg5MVNVhgKo6nOSStvxq4Ime8TOt9mabnls/TpJtdPckmJiYoNPpDLThEyvhliuODjR2WINu87BmZ2dHtu5RGLd+Yfx6Hrd+YfF7HiYQzgHeC3ysqp5M8lu0w0Pz6HdeoE5QP75YtRPYCTA5OVlTU1OntMHH3LV7L3ccGOpy0gN78fqpkay30+kw6Ou1HI1bvzB+PY9bv7D4PQ9zDmEGmKmqJ9vjh+kGxKvtMBDt/kjP8mt7xq8BXmn1NX3qkqQlNPA/k6vqT5K8nOQnq+oQcDXwbLttBW5r93vbkH3Af0nyaeCv0z15/FRVvZXk9SQbgSeBG4C7Bu7oDLdu+xdGst77N503kvVKWj6GPW7yMWB3krcBfwz8M7p7HXuS3Ai8BFwHUFUHk+yhGxhHgZur6q32PDcB9wMrgUfaTZK0hIYKhKr6GjDZZ9bV8yy/A9jRp74fuHyYbZEkDcdvKkuSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQJOQyAkWZHkD5L89/b4wiSPJnm+3V/Qs+ytSaaTHEpyTU/9yiQH2rw7k/S7zrIkaRGdjj2EjwPP9TzeDjxWVRuAx9pjklwKbAEuAzYBdydZ0cbcA2yje1nNDW2+JGkJDRUISdYAHwR+u6e8GdjVpncB1/bUH6qqN6rqBWAauCrJKuD8qnq8qgp4oGeMJGmJDHtN5X8H/Brwzp7aRFUdBqiqw0kuafXVwBM9y8202pttem79OEm20d2TYGJigk6nM9BGT6yEW644OtDY5Wp2dnbg12s5Grd+Yfx6Hrd+YfF7HjgQkvwicKSqnk4ydTJD+tTqBPXji1U7gZ0Ak5OTNTV1Mqs93l2793LHgWGzcHm5f9N5DPp6LUedTmes+oXx63nc+oXF73mYv4rvAz6U5APAjwLnJ/nPwKtJVrW9g1XAkbb8DLC2Z/wa4JVWX9OnLklaQgOfQ6iqW6tqTVWto3uy+EtV9VFgH7C1LbYV2Num9wFbkpybZD3dk8dPtcNLryfZ2D5ddEPPGEnSElmM4ya3AXuS3Ai8BFwHUFUHk+wBngWOAjdX1VttzE3A/cBK4JF2kyQtodMSCFXVATpt+n8DV8+z3A5gR5/6fuDy07EtkqTB+E1lSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJwBCBkGRtkt9P8lySg0k+3uoXJnk0yfPt/oKeMbcmmU5yKMk1PfUrkxxo8+5sV06TJC2hYfYQjgK3VNXfAjYCNye5FNgOPFZVG4DH2mPavC3AZcAm4O4kK9pz3QNso3tZzQ1tviRpCQ1zTeXDVfXVNv068BywGtgM7GqL7QKubdObgYeq6o2qegGYBq5Ksgo4v6oer6oCHugZI0laIqflHEKSdcDPAE8CE1V1GLqhAVzSFlsNvNwzbKbVVrfpuXVJ0hIa+prKSd4B/A7wq1X15yc4/N9vRp2g3m9d2+geWmJiYoJOp3PK2wswsRJuueLoQGOXq9nZ2YFfr+Vo3PqF8et53PqFxe95qEBI8iN0w2B3VX2+lV9NsqqqDrfDQUdafQZY2zN8DfBKq6/pUz9OVe0EdgJMTk7W1NTUQNt91+693HFg6CxcVu7fdB6Dvl7LUafTGat+Yfx6Hrd+YfF7HuZTRgHuBZ6rqk/3zNoHbG3TW4G9PfUtSc5Nsp7uyeOn2mGl15NsbM95Q88YSdISGeafye8D/ilwIMnXWu3fALcBe5LcCLwEXAdQVQeT7AGepfsJpZur6q027ibgfmAl8Ei7SZKW0MCBUFX/k/7H/wGunmfMDmBHn/p+4PJBt0WSNDy/qSxJAk7Dp4y0PBz41nf5pe1fGMm6X7ztgyNZr6RT4x6CJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNP12hRbduBD+ZccsVR5la8rVKy5t7CJIkwECQJDUGgiQJOIMCIcmmJIeSTCfZPurtkaRxc0acVE6yAvgPwM8BM8BXkuyrqmdHu2VazkZxMhu8/oOWrzNlD+EqYLqq/riqfgA8BGwe8TZJ0lg5I/YQgNXAyz2PZ4C/O3ehJNuAbe3hbJJDA67vYuC1AccuS78yZj2Pst/cPoq1AmP2HjN+/cLp6flvzDfjTAmE9KnVcYWqncDOoVeW7K+qyWGfZzkZt57HrV8Yv57HrV9Y/J7PlENGM8DansdrgFdGtC2SNJbOlED4CrAhyfokbwO2APtGvE2SNFbOiENGVXU0yS8D/wNYAdxXVQcXcZVDH3Zahsat53HrF8av53HrFxa551Qdd6hekjSGzpRDRpKkETMQJEnAGAbC2foTGUleTHIgydeS7G+1C5M8muT5dn9Bz/K3ttfgUJJrRrflJy/JfUmOJHmmp3bKPSa5sr1W00nuTNLvY88jN0+/v57kW+19/lqSD/TMW+79rk3y+0meS3Iwycdb/Wx+j+freTTvc1WNzY3uCetvAu8G3gZ8Hbh01Nt1mnp7Ebh4Tu03gO1tejtwe5u+tPV+LrC+vSYrRt3DSfT4fuC9wDPD9Ag8Bfws3e+/PAL8wqh7O4V+fx34RJ9lz4Z+VwHvbdPvBP6o9XU2v8fz9TyS93nc9hDG7ScyNgO72vQu4Nqe+kNV9UZVvQBM031tzmhV9WXg23PKp9RjklXA+VX1eHX/L3qgZ8wZZZ5+53M29Hu4qr7apl8HnqP7KwZn83s8X8/zWdSexy0Q+v1Exole/OWkgC8mebr9xAfARFUdhu5/eMAlrX42vQ6n2uPqNj23vpz8cpJvtENKxw6fnFX9JlkH/AzwJGPyHs/pGUbwPo9bIJzUT2QsU++rqvcCvwDcnOT9J1j2bH4djpmvx+Xe+z3AjwN/GzgM3NHqZ02/Sd4B/A7wq1X15ydatE/tbOl5JO/zuAXCWfsTGVX1Srs/Avwu3UNAr7ZdSdr9kbb42fQ6nGqPM216bn1ZqKpXq+qtqvoL4D/yl4f6zop+k/wI3T+Mu6vq8618Vr/H/Xoe1fs8boFwVv5ERpLzkrzz2DTw88AzdHvb2hbbCuxt0/uALUnOTbIe2ED3hNRydEo9tkMOryfZ2D6FcUPPmDPesT+MzT+h+z7DWdBv2757geeq6tM9s87a93i+nkf2Po/6LPtS34AP0D2T/03gk6PentPU07vpfvLg68DBY30BFwGPAc+3+wt7xnyyvQaHOEM/gdGnzwfp7j6/SfdfRDcO0iMw2f4H+ybw72nf2D/TbvP0+1ngAPCN9sdh1VnU79+ne5jjG8DX2u0DZ/l7PF/PI3mf/ekKSRIwfoeMJEnzMBAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTm/wHiPbNHAl669wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# la longueur des reviews\n",
    "reviews_len = list(map(len, reviews_num))\n",
    "pd.Series(reviews_len).hist();\n",
    "pd.Series(reviews_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nombre de review de taille 0 : une seule\n",
    "Counter(reviews_len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de la review de taille 0\n",
    "idx0 = np.where(np.array(reviews_len)==0)[0][0]\n",
    "del reviews_num[idx0]\n",
    "del labels_num[idx0]\n",
    "\n",
    "# autre méthode un peu plus lente\n",
    "#reviews_num = [reviews_num[i] for i,l in enumerate(reviews_len) if l>0]\n",
    "#labels_num = [labels_num[i] for i,l in enumerate(reviews_len) if l> 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la fonction padding/truncating avec en sortie un array de taille (nombre_de_reviews, taille_fixée_des_séquences)\n",
    "seq_len = 500 # la variable seq_len contient pour la suite la longueur fixée des séquences\n",
    "def trunc_pad(review_list=reviews_num, length=seq_len):\n",
    "    output = np.zeros((len(review_list), length), dtype = int)\n",
    "    \n",
    "    for i,rvw in enumerate(review_list):\n",
    "        rvw_len = len(rvw)\n",
    "        output[i,-rvw_len:] = np.array(rvw)[:length]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0]\n",
      " [    0     0     0     0     0]\n",
      " [22382    42 46418    15   706]\n",
      " [ 4505   505    15     3  3342]\n",
      " [    0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# test et affichage\n",
    "print(trunc_pad(length=250)[:5,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 469 ms, sys: 29.8 ms, total: 498 ms\n",
      "Wall time: 465 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = pad_sequences(reviews_num, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 410 ms, sys: 51.4 ms, total: 462 ms\n",
      "Wall time: 460 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = trunc_pad(review_list=reviews_num, length=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Échantillons d'entraînement, de validation et de test**\n",
    "\n",
    "Découper les données en train, validation et test sets de la manière qui vous plaira. Tant que c'est juste et cohérent bien sûr.  \n",
    "Il faut qu'il y ait 20000 observations dans le train, 2500 dans le validation et 2500 dans le test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 500), (25000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labels_num)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.7 ms, sys: 15.9 ms, total: 51.6 ms\n",
      "Wall time: 51.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# méthode sklearn\n",
    "X2, X_test, y2, y_test = train_test_split(X, y, test_size = 2500)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X2, y2, test_size = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 15.9 ms, total: 31.7 ms\n",
      "Wall time: 30.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# méthode \"à la main\" : on a 25000 observations, on en met 20000 dans le train, 2500 dans le validation et 2500 dans le test\n",
    "# sans oublier d'introduire de l'aléatoire : pour ça on va shuffle les vecteurs X et y mais attention il faut les mélanger de la même manière...\n",
    "idx_shuffled = np.arange(25000)\n",
    "np.random.shuffle(idx_shuffled)\n",
    "X_shuffled = X[idx_shuffled]\n",
    "y_shuffled = y[idx_shuffled]\n",
    "\n",
    "train_size = 20000\n",
    "X_train, X2 = X_shuffled[:train_size], X_shuffled[train_size:]\n",
    "y_train, y2 = y_shuffled[:train_size], y_shuffled[train_size:]\n",
    "\n",
    "test_size = 2500\n",
    "X_val, X_test = X2[:test_size], X2[test_size:]\n",
    "y_val, y_test = y2[:test_size], y2[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 500) (2500, 500) (2500, 500)\n",
      "(20000,) (2500,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "# on check les shapes\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Petite paranthèse sur le _word embedding_**\n",
    "\n",
    "Le [*word embedding*](https://fr.wikipedia.org/wiki/Word_embedding) consiste à transformer des mots sous forme de vecteurs de nombres. Bon ça on peut le faire relativement facilement, vous venez d'ailleurs de le faire avec la représentation du lexique des reviews en nombres entiers. Pour passer à un vecteur il suffirait juste de faire un one-hot-encoding.\n",
    "\n",
    "Il y a plusieurs problèmes à cette solution (même si on l'utilise parfois dans du NLP simple) :\n",
    "- la dimension de l'espace engendré\n",
    "- l'absence totale de notion de similarité (avec la représentation one-hot, 2 mots qui n'ont rien à voir sont aussi différents que 2 mots tout à fait synonyme)\n",
    "- le fait que les vecteurs contiennent quasiment que des 0 (sparse matrix)\n",
    "\n",
    "Donc le *word embedding* s'attaque à ce problème avec pour objectifs de :\n",
    "1. représenter les mots sous forme de vecteurs de nombres réels (et non entiers, on passe donc à un espace continu et plus discret)\n",
    "2. conserver la notion de similarité c'est-à-dire que 2 vecteurs qui sont proches doivent représenter des mots \"sémantiquement proches\"\n",
    "3. de les représenter dans un espace de plus petite dimension (en fonction de votre vocabulaire, soit le nombre de mots dans votre problème, ça peut aller très vite, généralement plusieurs milliers ou 10aines de milliers)\n",
    "\n",
    "Le principe est de s'intéresser au contexte des mots, c'est-à-dire, quels mots sont associés ensemble en s'appuyant sur la co-occurrence.\n",
    "Différents modèles de word embedding existent : [word2vec](https://fr.wikipedia.org/wiki/Word2vec), \n",
    "\n",
    "En pratique avec `keras`, que se passe-t-il lorsqu'on ajout une couche [*embedding*](https://keras.io/api/layers/core_layers/embedding/) ? Un exemple juste en dessous.\n",
    "\n",
    "Un peu de visionnage pour y voir plus clair si ça vous intéresse :\n",
    "- https://www.youtube.com/watch?v=Eku_pbZ3-Mw\n",
    "- https://www.youtube.com/watch?v=oUpuABKoElw\n",
    "- https://www.youtube.com/watch?v=5PL0TmQhItY\n",
    "\n",
    "Et un peu de lecture :\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2\n",
    "- https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4\n",
    "- https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "- https://medium.com/rasa-blog/supervised-word-vectors-from-scratch-in-rasa-nlu-6daf794efcd8\n",
    "- https://web.stanford.edu/~jurafsky/slp3/6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on prend quelques phrases\n",
    "t1 = \"i hope to see you again\"\n",
    "t2 = \"you wish to see me soon\"\n",
    "t3 = \"i wish we will meet again\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'i': 0,\n",
       "  'to': 1,\n",
       "  'see': 2,\n",
       "  'you': 3,\n",
       "  'again': 4,\n",
       "  'wish': 5,\n",
       "  'hope': 6,\n",
       "  'me': 7,\n",
       "  'soon': 8,\n",
       "  'we': 9,\n",
       "  'will': 10,\n",
       "  'meet': 11},\n",
       " 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# si on reprend en gros nos étapes de preprocessing\n",
    "cnts = Counter(t1.split() + t2.split() + t3.split())\n",
    "dico = {w:i for i,w in enumerate(sorted(cnts, key=cnts.get, reverse=True))}\n",
    "dico, len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 6, 1, 2, 3, 4], [3, 5, 1, 2, 7, 8], [0, 5, 9, 10, 11, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# et donc on peut les représenter comme ça\n",
    "t1_int = [dico[w] for w in t1.split()]\n",
    "t2_int = [dico[w] for w in t2.split()]\n",
    "t3_int = [dico[w] for w in t3.split()]\n",
    "t1_int, t2_int, t3_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-0.02313391, -0.00737948],\n",
       "         [ 0.01848278, -0.01095483],\n",
       "         [ 0.0044451 ,  0.00905938],\n",
       "         [ 0.03042718, -0.01061307],\n",
       "         [ 0.0430409 , -0.00418841],\n",
       "         [-0.01192189,  0.02178173]]], dtype=float32),\n",
       " array([[[ 0.0430409 , -0.00418841],\n",
       "         [-0.01960553,  0.03789306],\n",
       "         [ 0.0044451 ,  0.00905938],\n",
       "         [ 0.03042718, -0.01061307],\n",
       "         [ 0.03431991,  0.02336348],\n",
       "         [-0.03446075,  0.03879006]]], dtype=float32),\n",
       " array([[[-0.02313391, -0.00737948],\n",
       "         [-0.01960553,  0.03789306],\n",
       "         [-0.02391529,  0.00860884],\n",
       "         [ 0.02560779,  0.00216025],\n",
       "         [ 0.02318671, -0.02602875],\n",
       "         [-0.01192189,  0.02178173]]], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on crée un modèle avec une couche embedding\n",
    "# input_dim : taille du vocabulaire = nombre de mots dans le lexique, ici c'est 9\n",
    "# output_dim : taille des vecteurs utilisée pour représenter les mots, ici 2 car on a que 9 mots mais on choisit souvent 32/64 selon la taille du vocabulaire\n",
    "# input_length : taille des séquences d'input, ici c'est 6\n",
    "model_emb = Sequential()\n",
    "model_emb.add(Embedding(input_dim=12, output_dim=2, input_length=6))\n",
    "\n",
    "model_emb.compile('rmsprop', 'mse')\n",
    "out1 = model_emb.predict([t1_int]) # les crochets pour ajouter une dimension car le modèle est entraîné sur des inputs à 2 dimensions\n",
    "out2 = model_emb.predict([t2_int])\n",
    "out3 = model_emb.predict([t3_int])\n",
    "out1, out2, out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Création, entraînement et évaluation du modèle**\n",
    "\n",
    "Créer un premier modèle, que vous serez tout à fait libre et même cordialement conviés à améliorer par la suite, avec :\n",
    "- une couche Embedding\n",
    "- une couche LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 64)           4740672   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,806,773\n",
      "Trainable params: 4,806,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=len(word_mapping)+1, output_dim=64, input_length=seq_len))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 668s 1s/step - loss: 0.4631 - accuracy: 0.7829 - val_loss: 0.3629 - val_accuracy: 0.8464\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 615s 985ms/step - loss: 0.2630 - accuracy: 0.8992 - val_loss: 0.3006 - val_accuracy: 0.8768\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 632s 1s/step - loss: 0.1466 - accuracy: 0.9488 - val_loss: 0.3395 - val_accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7dfe5d54f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8668000102043152\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
